{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Crawling demo.ipynb","provenance":[],"collapsed_sections":["_hV4D9JSsocH"],"mount_file_id":"1ZDziZ2yOm3UNjAbZpxdYSs7NlEwlz8cU","authorship_tag":"ABX9TyNv2/Ytfy9zWGyMvohCxs55"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"_hV4D9JSsocH"},"source":["### **Installations and dependencies**"]},{"cell_type":"code","metadata":{"id":"a-rM84krugHT"},"source":["# change to local directory\n","import os\n","os.chdir(\"/content/drive/My Drive/Colab Notebooks/Demo capstone/Crawling\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lFhpow8Uvzhr","executionInfo":{"status":"ok","timestamp":1616177800544,"user_tz":420,"elapsed":8896,"user":{"displayName":"Viet Hoang Tran Duong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT3sg-hdUPlX9VamfzW6qUngDvYX0YJmdFBMNS5Q=s64","userId":"14828536041389094926"}},"outputId":"b2b96594-9a7d-4b4b-94e3-e2dc7a1c3ae5"},"source":["!pip install newspaper3k\n","!pip install aylien-apiclient\n","!pip install fake-useragent"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: newspaper3k in /usr/local/lib/python3.7/dist-packages (0.2.8)\n","Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n","Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n","Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (6.0.2)\n","Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.1.0)\n","Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13)\n","Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (0.3)\n","Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.0.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.1)\n","Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n","Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (0.0.4)\n","Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (1.1.0)\n","Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n","Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (0.35.1)\n","Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.10)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n","Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.5.3->newspaper3k) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2020.12.5)\n","Requirement already satisfied: aylien-apiclient in /usr/local/lib/python3.7/dist-packages (0.7.0)\n","Requirement already satisfied: httplib2>=0.9 in /usr/local/lib/python3.7/dist-packages (from aylien-apiclient) (0.17.4)\n","Requirement already satisfied: fake-useragent in /usr/local/lib/python3.7/dist-packages (0.1.11)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5JMTsoCTwCnG","executionInfo":{"status":"ok","timestamp":1616177801396,"user_tz":420,"elapsed":9739,"user":{"displayName":"Viet Hoang Tran Duong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT3sg-hdUPlX9VamfzW6qUngDvYX0YJmdFBMNS5Q=s64","userId":"14828536041389094926"}},"outputId":"648fd2c9-a120-4cc1-e974-9b66520a88e3"},"source":["# importing packages\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4NzHN61yukl-","executionInfo":{"status":"ok","timestamp":1616177808653,"user_tz":420,"elapsed":16986,"user":{"displayName":"Viet Hoang Tran Duong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT3sg-hdUPlX9VamfzW6qUngDvYX0YJmdFBMNS5Q=s64","userId":"14828536041389094926"}},"outputId":"ad066c5a-67b4-4388-e9bc-2dffcf3ac1d0"},"source":["from get_content.read_url import read_url\n","from get_content.get_medium import get_medium\n","from get_content.get_query import get_query\n","from get_content.get_arxiv import get_arxiv\n","from get_content.get_neurips import get_neurips\n","from datetime import datetime"],"execution_count":null,"outputs":[{"output_type":"stream","text":["start loading vectorization\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cFUI7iSEqe6x"},"source":["### **1. Read content from any url**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"id":"0CznZX0NvyNu","executionInfo":{"status":"ok","timestamp":1616177809077,"user_tz":420,"elapsed":17400,"user":{"displayName":"Viet Hoang Tran Duong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT3sg-hdUPlX9VamfzW6qUngDvYX0YJmdFBMNS5Q=s64","userId":"14828536041389094926"}},"outputId":"bd1d6909-351c-493c-d03a-2081da766ed9"},"source":["read_url(\"https://www.cnbc.com/2021/03/19/-first-us-china-meeting-under-biden-gets-off-to-a-rocky-start.html\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'U.S. Secretary of State Antony Blinken (2nd R), joined by national security advisor Jake Sullivan (R), speaks while facing Yang Jiechi (2nd L), director of the Central Foreign Affairs Commission Office, and Wang Yi (L), China\\'s foreign minister at the opening session of U.S.-China talks at the Captain Cook Hotel in Anchorage, Alaska on March 18, 2021. Frederic J. Brown | AFP | Getty Images\\n\\nBEIJING — The first high-level gathering of U.S. and Chinese officials under President Joe Biden kicked off with an exchange of insults at a pre-meeting press event in Alaska on Thursday. A planned four-minute photo session for the officials to address reporters ended up lasting one hour and 15 minutes due to a frothy exchange, according to NBC News. Both the Chinese and U.S. side kept calling the reporters back into the room so they could add remarks. Expectations were already low for the meeting in Anchorage, Alaska, with U.S. Secretary of State Antony Blinken, National Security Advisor Jake Sullivan, Chinese Foreign Minister Wang Yi and Yang Jiechi, director of the Central Foreign Affairs Commission of the Chinese Communist Party. In his opening remarks, Blinken said the U.S. would discuss its \"deep concerns with actions by China, including in Xinjiang, Hong Kong, Taiwan, cyber attacks on the United States, economic coercion toward our allies.\" \"Each of these actions threaten the rules-based order that maintains global stability. That\\'s why they\\'re not merely internal matters, and why we feel an obligation to raise these issues here today,\" Blinken said. \"I said that the United States relationship with China will be competitive where it should be, collaborative where it can be, adversarial where it must be.\"\\n\\nThe United States does not have the qualification to say that it wants to speak to China from a position of strength. Yang Jiechi director of the Central Foreign Affairs Commission\\n\\nBeijing considers issues in Xinjiang, Hong Kong and Taiwan as part of its domestic affairs, and the officials reiterated at the meeting that China is firmly opposed to foreign interference. Yang said the U.S. side \"carefully orchestrated\" the dialogue, according to an official translation reported by NBC. \"I think we thought too well of the United States, we thought that the U.S. side will follow the necessary diplomatic protocols,\" Yang said, adding that \"the United States does not have the qualification to say that it wants to speak to China from a position of strength.\" Yang said the U.S. must deal with the Chinese side in \"the right way\" and reiterated Beijing\\'s call for cooperation.\\n\\nI\\'m hearing deep satisfaction that the United States is back, that we\\'re reengaged with our allies and partners. I\\'m also hearing deep concern about some of the actions your government is taking. Antony Blinken U.S. Secretary of State\\n\\nUnder Chinese President Xi Jinping, the Chinese government has been consolidating its power at home and abroad. In the last year, Beijing has pushed ahead with major trade deals with Asia-Pacific neighbors and the European Union. Chinese authorities have also emphasized their success in quickly controlling the coronavirus pandemic domestically, and their claim of lifting all 1.4 billion people in the country out of poverty — both of which Yang pointed to in his meeting with U.S. officials. \"We believe that it is important for the United States to change its own image, and to stop advancing its own democracy in the rest of the world,\" Yang said. China\\'s Ministry of Foreign Affairs did not immediately have a comment. State-run broadcaster CCTV said the U.S. went \"seriously overtime\" in its opening remarks and \"provoked disputes,\" according to a CNBC translation of the Mandarin-language report.'"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"146-QEDSswAF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OL_kVXdDsY5Z"},"source":["### **2. Read the content from popular Medium publishers**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W7JReYEhwaf8","executionInfo":{"status":"ok","timestamp":1616177813147,"user_tz":420,"elapsed":21456,"user":{"displayName":"Viet Hoang Tran Duong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT3sg-hdUPlX9VamfzW6qUngDvYX0YJmdFBMNS5Q=s64","userId":"14828536041389094926"}},"outputId":"a939b480-1c45-4785-8b6e-db56cd9cf923"},"source":["start_date=datetime.strptime(\"2020-02-01\", \"%Y-%m-%d\")\n","end_date=datetime.strptime(\"2020-03-31\", \"%Y-%m-%d\")\n","\n","output, text = get_medium('Towards Data Science (AI/ML/Data)', start_date, end_date)\n","\n","print(\"Number of posts crawled:\", len(output))\n","print(\"Example of one crawled information:\", output[0])\n","print(\"\")\n","print(\"All the text of all posts (combined):\", text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of posts crawled: 10\n","Example of one crawled information: [15000, 'Why Python is not the programming language of the\\xa0future', 'https://towardsdatascience.com/why-python-is-not-the-programming-language-of-the-future-30ddc5339b66', ['Why Python is not the programming language of the future', 'What makes Python popular right now', 'Downsides of Python — and whether they’ll be fatal', 'What could replace Python in the future — and when']]\n","\n","All the text of all posts (combined): 7 Essential Tips for Writing With Jupyter Notebook Dynamic Time Warping Microsoft Excel in the era of big data Multiple Time Series Classification by Using Continuous Wavelet Transformation NLP in the Stock Market Web Scraping Yahoo Finance Cython-A Speed-Up Tool for your Python Function Transform Reality with Pandas Meena: Google’s New Chatbot Implementing 2D Physics in JavaScript Machine Learning with Datetime Feature Engineering: Predicting Healthcare Appointment No-Shows TensorFlow vs PyTorch for Deep Learning Is an AWS certification worth it? 5 Elegant Python Pandas Functions An intuitive explanation of Beam Search How to build a resume parsing tool Using Graph Convolutional Neural Networks on Structured Documents for Information Extraction Pool Limited Queue Processing in Python Build A Dashboard To Track The Spread of Coronavirus Using Dash Timing for Efficient Python Code Top 10 In-Demand programming languages to learn in 2020 How bad will the Coronavirus Outbreak get? — Predicting the outbreak figures Data Cleaning in Python: the Ultimate Guide (2020) The Ultimate Guide to Getting Started in Data Science 4 Hidden Python Features that Beginners should Know Top 10 Artificial Intelligence Trends for 2020 Top 20 Data Science Discord servers to join in 2020 Top Trends of Graph Machine Learning in 2020 4 useful tips of Pandas GroupBy 5 New Features in pandas 1.0 You Should Know About Transition from Mechanical Engineer to Machine Learning Engineer (or Data Scientist) Modelling the coronavirus epidemic spreading in a city with Python Understanding Latent Space in Machine Learning Manage your Python Virtual Environment with Conda 3 Reasons to Use Random Forest Over a Neural Network–Comparing Machine Learning versus Deep Learning Time Series Analysis with Auto.Arima in R How Confidence and Prediction intervals work 💡Illustrating the Reformer I Don’t Believe in Electrons Building an End-to-End Defect Classifier Application for Printed Circuit Boards Closures and Decorators in Python Single, Double, and Triple Quotes in Python The statistical foundations of machine learning No-Code/Low-Code AI: New Business Models and Future of Data Scientists The complete guide to clustering analysis How to visualize hidden relationships in data with Python — analysing NBA assists Stop copy-pasting notebooks, embrace Jupyter templates! Decision Tree: Build, prune and visualize it using Python How to Use Lambda for Efficient Python Code One Word to Define Each of the 20 Most Popular Programming Languages Could Nim Replace Python? The Amazon Data Scientist Interview Understanding Region of Interest — (RoI Pooling) Topic Modeling Quora Questions with LDA & NMF How to download files using Python A Machine Learning Project — Predicting Used Car Prices Implementing neural machine translation using keras Yield Curve Building In Python Upgrading Python lists Are You on Track? — Communicate Your Strategic Themes on an Interactive Roadmap with No-Code. 5 Common SQL Interview Problems for Data Scientists SVM From Scratch — Python Top 3 VS Code Extensions for Python and Data Science Is it possible to predict stock prices with a neural network? Introduction to Bayesian Logistic Regression Historical Stock Price Data in Python How to Save Money with Python Docker Best Practices for Data Scientists Building a Sentiment Classifier using Scikit-Learn Ignite the Spark! Logistic Regression Explained Generalists vs. Specialists in Data Science and Analytics Cache Patterns with Apache Spark Learn Machine Learning the Fun Way Automated Machine Learning using Python3.7: Improving Efficiency in Model Development Deep Learning vs. Machine Learning How to start a Data Science Project using Google Cloud Platform Visualizing My LinkedIn Network Power BI: Add Category ‘Other’ to Charts How Data Analysis Helps Unveil the Truth of Coronavirus Importing Data to Google Colab — the CLEAN Way Understanding Region of Interest — (RoI Align and RoI Warp) Interview strategy that landed me my first data science job Exploratory Data Analysis with Pandas Profiling How to overcome Coder’s Block Auto-Generated Knowledge Graphs What is the Data Architecture We Need? How To Make A Successful Switch To A Data Science Career Designing a Feature Selection Pipeline in Python Deep Learning for Time Series and why DEEP LEARNING? 3 Good Python Practices for Beginners 10 Stages Of A Machine Learning Project In 2020 (And Where You Fit) An R Package to Explore the Novel Coronavirus Web Scraping Using Python and BeautifulSoup Machine Learning: Step-By-Step Cluster-then-predict for classification tasks Building a Data Warehouse: Basic Architectural principles Top US Colleges for Computer Science Basic Algorithms — Finding the Closest Pair Implementing the General Tree and Depth-First-Search (DFS) in python! (from scratch) Sexiest Job but…. A Data Scientist’s Perspective on the Coronavirus Outbreak Sales Forecasting with Price & Promotion effects Working with Azure Cosmos DB in your Azure Functions Decision Trees and Random Forests — Explained Chi-Squared Test for Feature Selection with implementation in Python Building a real-time prediction pipeline using Spark Structured Streaming and Microservices How does the Bellman equation work in Deep RL? tl;dr: Dirichlet Process Gaussian Mixture Models made easy. Python + Azure Cosmos DB The “Jira mindset” is damaging your data science team Quantile Regression Handling Missing Values with Pandas The Apple Data Scientist Interview Fun Valentine’s Day “Gift” Ideas For Python Programmers Journey into the Cloud: How to Become a AWS Certified Solution Architect From Time Series to Supervised Learning Go beyond the basics of the request package in python Using Classes for Machine Learning Virtual Environments for Data Science: Running Python and Jupyter with Pipenv 40 Statistics Interview Problems and Answers for Data Scientists Outstanding results predicting Apple Stock applying ML on Global News (Python) Looking Beyond the Hype: Is Modular Monolithic Software Architecture Really Dead? Affordable Coffee Solubility Tools (TDS) for Espresso: Brix vs Atago How to implement a Gaussian Naive Bayes Classifier in Python from scratch? Energy consumption time series forecasting with python and LSTM deep learning model Hadoop & The Raspberry Pi: My Final Year Project Detecting animals in the backyard — practical application of deep learning. How To Use Python To Buy Options From Robinhood Entity Level Evaluation for NER Task Top 20 free Data Science, ML and AI MOOCs on the Internet Which party adds more to deficits? Depth Estimation: Basics and Intuition Comprehensive Guide To Approximate Nearest Neighbors Algorithms When to Use the Kolmogorov-Smirnov Test Design Patterns for MongoDB Dictionary as an Alternative to If-Else Chasing the Data: Coronavirus My First Day As A Computer Vision Engineer A Data Scientists Guide to Python Modules and Packages Bayes’ Rule, Decision Making, And Containing COVID-19 With Unreliable Diagnostic Tests How to Fight the Coronavirus with AI and Data Science What is the difference between a having clause and a where clause in SQL? Pytorch [Basics] — Intro to RNN An introduction to Graph Neural Networks Tour of Python Itertools How to Tokenize Tweets with Python A Simple Guide to A/B Testing for Data Science Creating Beautiful Sankey Diagrams with floWeaver How to secure Azure Functions with Azure AD, Key Vault and VNETs How I Learned Python in 6 Months The easiest way to download YouTube videos using Python Magic Methods in Python, by example Why Euler’s Formula for Primes could disrupt the World Build a custom-trained object detection model with 5 lines of code Data science is becoming software engineering A better way for asynchronous programming: asyncio over multi-threading Generating new faces with Variational Autoencoders Exploring Moving Averages to Build Trend Following Strategies in Python How to Learn Geospatial data science for free in 2020 Python ETL Tools: Best 8 Options An Overview Of Importing Data In Python CRISP-DM methodology leader in data mining and big data Gradient Boosted Decision Trees-Explained implementing Neural Machine Translation with Attention using Tensorflow Reordering Pandas DataFrame Columns: Thumbs Down On Standard Solutions Everything you Need to Know About Web Scraping Getting Started with AutoKeras Understanding the Bias-Variance Tradeoff and visualizing it with example and python code Create effective data visualizations of proportions Why The Coronavirus Mortality Rate is Misleading Ten Tips to Save you Time and Frustration When Programming ⏳ The mathematics of optimization for deep learning Deploying a Simple Machine Learning Model into a WebApp using TensorFlow.js “My data scientist doesn’t know how to properly start an EC2 instance”. How the 80/20 Rule can help decide which skills you need to start a career in Data Science Face Detection on Custom Dataset with Detectron2 and PyTorch using Python What is statistical bias and why is it so important in data science? Verifying and Tackling the Assumptions of Linear Regression First Order Motion Model How to deploy Apache Airflow with Celery on AWS Regularization in Deep Learning — L1, L2, and Dropout The Forgotten Algorithm Recommender Systems in Python from Scratch! Collecting Data for Custom Object Detection Optimize Python Code in Jupyter Notebook Building a Deep Learning Person Classifier Data Science Resume Mistakes to Avoid Logistic Regression — Explained Beyond BERT? How to use NLP in Python: a Practical Step-by-Step Example When to use CPUs vs GPUs vs TPUs in a Kaggle Competition? Data Scientist: The Dirtiest Job of the 21st Century Ranking Presidential Candidates By Effectiveness At Getting Bills Enacted AI = “Automated Inspiration” Here’s How to Read License Plate with 10 Lines of Python How to Make Instagram Unfollower Tool with Python A Quick Introduction to Google Earth Engine Find stocks worth buying with Machine Learning How I used machine learning to strategize my GRE preparation. Using Kalman Filter to Predict Coronavirus Spread How to Learn Data Science when Life does not Give You a Break Using Python and Robinhood to Build An Iron Condor Options Trading Bot Training Tensorflow Object Detection API with custom dataset for working in Javascript and Vue.js Behind the Coronavirus Mortality Rate Google Hash Code 2020: a greedy approach How I became an AI Consultant: Interview Questions & Answers Deep learning image enhancement insights on loss function engineering Best Practices for Airflow Developers USA Accidents Data Analysis I built a DIY license plate reader with a Raspberry Pi and machine learning Getting Started With Jupyter Notebooks in Visual Studio Code Deep Learning using GPU on your MacBook Scrapy: This is how to successfully login with ease How to Speedup your Pandas Code by 10x Three Major Fields of Artificial Intelligence and Their Industrial Applications Google Translate API for Python Web Scraping News Articles to Build an NLP Data Pipeline How to Enter Your First Kaggle Competition Data Clustering Tutorial for Advanced Ten Tricks To Speed Up Your Python Codes Please Stop Doing These 5 Things in Pandas Supercharging MS SQL Server with Python Deepnote Sets Course to Become the Jupyter Killer A New Definition of Data Science in Academic Programs Introduction to Probabilistic Graphical Models Checkpointing Deep Learning Models in Keras Exploring SimCLR: A Simple Framework for Contrastive Learning of Visual Representations Introduction to Data Engineering Saving and Loading Keras model using JSON and YAML files The Netflix Data Scientist Interview 15 things you should know about Dictionaries in Python Why a major AI Revolution is coming, but it’s not what you think — AAAI 2020 What are the Top 10 Data Science and AI Books of 2020 Convolutional Neural Networks’ mathematics Neo4j vs GRAKN Part I: Basics Recommendation System Series Part 4: The 7 Variants of MF For Collaborative Filtering How to stream video with real-time object detection on Raspberry Pi How to Learn Programming Like Einstein Learned Physics Demystifying Scrapy Item Loaders A table detection, cell recognition and text extraction algorithm to convert tables in images to excel files Nvidia gave me a $15K Data Science Workstation — here’s what I did with it This Will Make You a Command-Line Ninja p-value Basics with Python Code How to use Docker to deploy a Dashboard app on AWS Is Flux Better Than Tensorflow? Effective Data Filtering in Pandas Using .loc[] Face Recognition using Deep Learning Python for Finance — Stock Price Trend Analysis Review: Deep Learning In Drug Discovery Amazon’s Data Scientist Interview Practice Problems Recommendation System in Python: LightFM Customer Segmentation with Machine Learning Panel data regression: a powerful time series modeling technique Latent Semantic Analysis — Deduce the hidden topic from the document Processing and visualizing multiple categorical variables with Python — NBA’s schedule challenges How to embed a Spark ML Model as a Kafka Real-Time Streaming Application for Production Deployment SFM Self Supervised Depth Estimation: Breaking Down The Ideas Ball Detection with Computer Vision AI in Sports Deep Learning Algorithms — The Complete Guide From PyTorch to PyTorch Lightning — A gentle introduction Cohen’s Kappa How top CS students problem solve How I Created an Interactive, Scrolling Visualisation with D3.js, and how you can too Time Series Analysis with Pandas Writing good SQL Machine learning for stock prediction. A quantitative approach SQL — Practical Details Cheat Sheet for Data Analysis Creating a Serverless Python Chatbot API in Microsoft Azure from Scratch in 9 Easy Steps Ten Time-Saving R Hacks 8 Common Data Structures every Programmer must know Unsupervised NER using BERT Pros and cons of various Classification ML algorithms Deep Transfer Learning for Image Classification What is “Ground Truth” in AI? (A warning.) Python Numba or NumPy: understand the differences Text Cleaning Methods for Natural Language Processing Beyond A/B Testing: Primer on Causal Inference Hypothesis Testing Explained as Simply as Possible How to tackle any classification problem end to end & choose the right classification ML algorithm. Build a web data dashboard in just minutes with Python Google Trends API for Python Complete Guide to Data Visualization with Python PyTorch [Tabular] — Binary Classification Top Google AI Tools for Everyone The Binomial Regression Model: Everything You Need to Know Visualising Graph Data with Python-igraph Data Analysis & Visualization in Finance — Technical Analysis of Stocks using Python How To Train Your Chatbot With Simple Transformers Improving sentence embeddings with BERT and Representation Learning Fast & Asynchronous in Python Automating Every Aspect of Your Python Project Understanding Probability And Statistics: The Essentials Of Probability For Data Scientists What your favorite text editor reveals about your personality What is Operations Research? Machine learning beats BTC/USDT on unseen data, even with transaction fees and slippage. Using CTEs to Improve SQL Queries Understanding Batch Normalization for Neural Networks Time series forecasting with AdaBoost, random forests and XGBoost Rookie errors in machine learning Why Is Python Called Python? Coronavirus data visualizations using Plotly Deep Learning Hardware: Know Your Options Here’s where you can see Live Updates and Statistics on the Coronavirus Interactive Geospatial Data Visualization with Geoviews in Python What are Data Science recruiters looking for in a resume? Introduction to Convolutional Neural Network (CNN) using Tensorflow Train a GPT-2 Transformer to write Harry Potter Books! 9 Coronavirus Research Trends using LDA and Topic Modelling Swish: Booting ReLU from the Activation Function Throne Why jK8v!ge4D isn’t a good password An Introduction to Support Vector Regression (SVR) Contrastive Loss Explained Fascinating Relationship between AI and Neuroscience What is Gradient Clipping? The 9 concepts and formulas in probability that every data scientist should know K-Means Clustering — Explained Serverless: Building Your Own Router Google’s EfficientDet: An Overview Bayesian Hierarchical Modeling (or “more reasons why autoML cannot replace Data Scientists yet”) Architecture for High-Throughput Low-Latency Big Data Pipeline on Cloud tSNE vs. UMAP: Global Structure Named Entity Recognition (NER) with BERT in Spark NLP Four reasons why everyone except for Computer Scientists writes sloppy code Getting started with APIs in Python to Gather Data Don’t Worry, Excel is Surprisingly Effective Mastering Dates and Timestamps in Pandas (and Python in general) How to Run Apache Airflow as Daemon Using Linux “systemd” Top 25 Selenium Functions That Will Make You Pro In Web Scraping How To Find Decision Tree Depth via Cross-Validation Understanding Python Bytecode An Introduction to Graph Neural Network(GNN) For Analysing Structured Data Why I Left a Job I Loved in AI/ML The impact of COVID-19 — data visualization using Plotly and comparative analysis with SARS Creating Word Embeddings: Coding the Word2Vec Algorithm in Python using Deep Learning Best Apps To Learn Data Science in 2020 Building a ResNet in Keras MLOps with a Feature Store Concrete Compressive Strength Prediction using Machine Learning Exploring the Coronavirus Dataset I Thought I Was Mastering Python Until I Discovered These Tricks The Most Useful ML Tools 2020 Time Series Analysis for Machine Learning Speed up your Data Analysis with Python’s Datatable package What is Transposed Convolutional Layer? Setting up text preprocessing pipeline using scikit-learn and spaCy Why Do Boomers Still Run Everything? Label Propagation Demystified Predictive Analysis of an IPL Match What your terminal colors reveal about your personality 12 Amazing Pandas & NumPy Functions Top 10 Free Automation Software That Will Make Your Life Easier Comprehensive Guide on Item Based Recommendation Systems Modelling Binary Logistic Regression Using Python (research-oriented modelling and interpretation) — One Zero Blog Implementing Causal Inference: Trying to Understand the Question of Why Understanding Maximum Likelihood Estimation (MLE) Intro to FastAI: Installation and Building our First Classifier Introduction to D-Tale Deep Learning on graphs: convolution is all you need Easily create an AI app for your phone — zero coding experience 9 Reasons why you’ll never become a Data Scientist Covid-19 infection in Italy. Mathematical models and predictions How To Track Coronavirus In Your Country with Python My top 5 visualization tools for data science 12 Colab Notebooks that matter Object Detection using YoloV3 and OpenCV Bayes’ rule with a simple and practical example Decision Trees Explained Architecture comparison of AlexNet, VGGNet, ResNet, Inception, DenseNet Analyzing Coronavirus (Covid-19) Data Using Pandas and Plotly Crack SQL Interviews Coronavirus and Probability — The media must learn how to report statistics now AI & The Future of Consulting: Will Data Scientists Become Consultants? Build a Super Simple GAN in PyTorch A True End-to-End ML Example: Lead Scoring Quickly extract all links from a web page using JavaScript and the browser console 15 Data Exploration techniques to go from Data to Insights Transformers are Graph Neural Networks Classifier calibration Comparing the best free financial market data APIs Transposed Convolution Demystified Understanding Mathematical Symbols with Code Guide to Big Data Joins — Python, SQL, Pandas, Spark, Dask Interactive Reporting in Jupyter Notebook Modelling Binary Logistic Regression Using R (research-oriented modelling and interpretation) — One Zero Blog Using AWS Sagemaker and Lambda to Build a Serverless ML Platform Self-Balancing Binary Search Trees 101 Coronavirus Google Trends in Python Multiple Linear Regression This Should Be the First Step of a Personal Data Science Project How to show all columns / rows of a Pandas Dataframe? Image Data Labelling and Annotation — Everything you need to know Estimating the Number of Future Coronavirus Cases in the United States Making Plots in Jupyter Notebook Beautiful & More Meaningful 8 Useful Tree Data Structures Worth Knowing Top 6 Data Science Books that you must study in 2020 How to query and calculate GA4 event data in BigQuery An introduction to Snowplow Bayesian Inference Algorithms: MCMC and VI See the Coronavirus for Yourself Top 5 R resources on COVID-19 Coronavirus Social Distancing to Slow the Coronavirus 6 SQL Tricks Every Data Scientist Should Know The Corona Map: Visualizing the Pandemic Coronavirus in the News: Are they Overreacting? Opinion: How Worried Should We Be Of The Coronavirus? PIMA Indian Diabetes Prediction Summarization has gotten commoditized thanks to BERT Microsoft’s Data Scientist Interview Questions Practical AI : Automatically Generate Multiple Choice Questions (MCQs) from any content with BERT Summarizer, Wordnet… A Heuristic Derivation of Einstein’s Gravity Equations Now We Know What Happens To Cryptocurrency In A Crash Smarter COVID-19 Decision-Making A Simple Way to Gather all Coronavirus Related Data with Python StyleGAN v2: notes on training and latent space exploration Google Just Introduced TensorFlow Developer Certificate Exam A girl’s guide to surviving Bay Area data science interviews Python Interactive Data Visualization with Altair Why the year 2020 will prove to be a headache for Data Scientists Hyperparameter Tuning with Python: Complete Step-by-Step Guide Top 3 Python Functions You Don’t Know About (Probably) Create a web app w/ Coronavirus case data in a blink of an eye — using R, Shiny and Plotly Human Pose Estimation with Stacked Hourglass Network and TensorFlow How I DIY’d my Budget Using Python for Selenium and Beautiful Soup Musical Genre Classification with Convolutional Neural Networks The Essential Guide to Creating an AI Product in 2020 A quick guide to distributed training with TensorFlow and Horovod on Amazon SageMaker Track Coronavirus in Your Country by Displaying Notifications Using Python Intro to R: Linear Algebra On the Beauty of Math Machine Learning for Biology: How Will COVID-19 Mutate Next? Visual Notes From Singapore’s First 100 Fully Recovered Covid-19 Patients Recursive SQL Queries with PostgreSQL The Ultimate Guide to AdaBoost, random forests and XGBoost Should I go to brunch? An interactive tool for COVID-19 curve flattening Training a TensorFlow Faster R-CNN Object Detection Model on Your Own Dataset How does XGBoost Work Using Deep Learning to detect NCOV-19 from X-Ray Images Classify Growth Patterns For COVID-19 Data Deep Learning Course notebooks worth $2,000 are now open source Top Programming Languages for AI Engineers in 2021 Boruta explained the way I wish someone explained it to me Making simple games in Python Deep Learning based Super Resolution with OpenCV Rookie Data Science Mistake Invalidates a Dozen Medical Studies Installing GitHub in Visual Studio Code for Windows 10 Understand and Build FP-Growth Algorithm in Python From Data Analyst to Data Storyteller in 3 Steps Scale Your Data Pipelines with Airflow and Kubernetes Data Scientists Must Know Probability Modeling Exponential Growth How to Pull Data from an API using Python Requests PyTorch [Tabular] —Multiclass Classification Machine Learning methods to aid in Coronavirus Response 11 best Data Science classes if you’re locked home because of Coronavirus Visualise COVID-19 case data using Python, Dash, and Plotly Two Pandas functions you must know for easy data manipulation in Python Do you make decisions rationally? Visualize the Pandemic with R #COVID-19 How to Build an Accounting System using SQLite COVID-19 Open Source Dashboard Coronavirus: What does the data say for Pakistan Guide to Interpretable Machine Learning Why most of you made an irrational decision The Matter Of AI Takeover: Will Artificial Intelligence Replace Human Beings? Window Functions In Pandas Estimating the Global Growth of Coronavirus How I got a job as a Data Scientist without a STEM background Filtering Lists in Python Create Music Recommendation System Using Python 20 Machine Learning Interview Questions You Must Know for Data Scientists How To Use Riot API With Python The Stages of Learning Data Science Completely Free Machine Learning Reading List How to build Spark from source and deploy it to a Kubernetes cluster in 60 minutes Will we survive the COVID-19 pandemic? Can Python dataviz libraries repeat the Tableau worksheet? Detectron2 : The bare basic end to end tutorial A Practical Guide for Data Analysis with Pandas Required Sample Size for A/B Testing Free Stock Data for Python Using Yahoo Finance API Detecting COVID-19 induced Pneumonia from Chest X-rays with Transfer Learning: An implementation in Tensorflow and… Data Structures in C++ — Part 1 The Maths behind Back Propagation AI Papers to Read in 2020 COVID-19 Radiology Dataset (chest XRay & CT) for Annotation & Collaboration (Part 1) Most Popular Convolutional Neural Networks Architectures COVID-19 with a Flair Why data normalization is important for non-linear classifiers Face Recognition/Raspberry Pi : your own security alarm system from sklearn import * How to Export Pandas DataFrame to CSV My Python Pandas Cheat Sheet 3 Steps to Forecast Time Series: LSTM with TensorFlow Keras Create Virtual Environment using “virtualenv” and add it to Jupyter Notebook Top 26 Data Science YouTube Channels you should subscribe to in 2020 [Updated… 5 Online Courses you can take for free during COVID-19 Epidemic Inventory Management using Python How to Use Data Science on the Stock Market The Quick and Easy Way to Plot Error Bars in Python Using Pandas 10 Python built-in functions you should know Building a Food Recommendation System Exploratory Data Analysis (EDA) Visualization Using Pandas 4 Reasons Why I’m Choosing Plotly as My Main Visualization Library How to Easily Process Audio on Your GPU with TensorFlow The Math behind Schrödinger Equation: The Wave-particle duality and the Heat equation. Web Applications in Python Machine Learning Experiment Tracking Why Logarithms Are So Important In Machine Learning Time Series Forecasting with Statistical Models in Python Code OVER 100 Data Scientist Interview Questions and Answers! Best Python Libraries for Machine Learning and Deep Learning Why are COVID-19 statistics so different for Germany and Italy? Why and When to Avoid S3 as a Data Platform for Data Lakes Overcoming confirmation bias during COVID-19 Accuracy of Coronavirus Tests My pick for top 48 advanced database systems interview questions 10 Must-read Machine Learning Articles (March 2020) Introduction to Bayesian Decision Theory Why Data Science Is The Greatest Skill I Ever Obtained A Complete Guide to Using Progress Bars in Python Review of GANs for tabular data Sample Load balancing solution with Docker and Nginx Visualizing the Stock Market with Python Bokeh Policy Iteration in RL: An Illustration (In-depth) Machine Learning Image Classification With TensorFlow How to Use DBSCAN Effectively How to Compare Large Files Computer Vision 101: Working with Color Images in Python Applying Agile Framework to Data Science Projects Jupyter is now a full-fledged IDE Automate These 3 (Boring!!) Excel Tasks with Python! A Friendly Introduction to Text Clustering How this A.I became a communist How to Use Decorators in Python, by example Annotate Your Image — Using Online Annotation Tool! Should We Stay in Data Science? Step-by-step guide on how to train GPT-2 on books using Google Colab COVID-19 & Machine Learning Wikipedia API for Python What do various countries’ healthcare capacities look like? 5 Datasets About COVID-19 you can Use Right Now 5 Premium Courses You Can Access For Free This Month CNN Transfer Learning & Fine Tuning BigQuery + Cloud Functions: how to run your queries as soon as a new Google Analytics table is available A Short Review of COVID-19 Data Sources Docker for Python-Dash & R-Shiny Just Used Machine Learning in My Workout! It’s Time to Familiarize Yourself With NoSQL Databases More Than Ever Investigation of Explainable Predictions of COVID-19 Infection from Chest X-rays with Machine Learning Are you using Python with APIs? Learn how to use a retry decorator! Modeling Logistic Growth Hidden Markov Model — Implemented from scratch Step by Step Implementation: 3D Convolutional Neural Network in Keras Variational Autoencoders (VAEs) for Dummies — Step By Step Tutorial How to Get Beautiful Results with Neural Style Transfer A Quick Deep Learning Recipe: Time Series Forecasting with Keras in Python Pytorch [Tabular] — Regression How testing completely skews Coronavirus case fatality rates Setting Up Your Data Science Work Bench 15 Python Libraries That A Data Scientist Need To Know Assessing the Impact of the Coronavirus Lockdown on our Environment through Data. Pandas equivalent of 10 useful SQL queries 25 Deep Learning Interview Questions How To Visualize the Coronavirus Pandemic with Choropleth Maps Forecasting COVID-19 cases in India Walkthrough: Mapping GIS Data in Python Are Stock Returns Normally Distributed? Ideal Python environment setup for Data Science Simple Explanation of Transformers in NLP 10 things you’re doing wrong in Java R - Statistical Programming Language Price Prediction using Machine Learning Regression — a case study Tracking Coronavirus(COVID-19) Spread in India using Python Boosting Showdown: Scikit-Learn vs XGBoost vs LightGBM vs CatBoost in Sentiment Classification Three Data Science Technologies to Explore while you Self-Isolate: What are Docker, Airflow and Elasticsearch? Detecting COVID-19 with 97% accuracy: beware of the AI hype! 20 Minute Data Science Crash Course for 2020 Ace the SQL Data Science Interview in less than 10 minutes 5 Tips for making the transition from Psychology to Data Science Why Python is not the programming language of the future Decision Trees for Classification: ID3 Algorithm Explained Realistic Deepfakes in 5 Minutes on Colab Combine LSTM and VAR for Multivariate Time Series Forecasting Data Science Concepts Explained to a Five-year-old 12 Examples of How To Write Better Code Using @dataclass My First Month As A Computer Vision Engineer Decision Tree Fundamentals Machine Learning Engineer versus Software Engineer Covid-19: The Second Wave In Singapore \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Bq-PxOlbswdn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BLedWnMHshIr"},"source":["### **3. Read content from Medium with any given search query**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"31YPo8QPxP7k","executionInfo":{"status":"ok","timestamp":1616177832646,"user_tz":420,"elapsed":40945,"user":{"displayName":"Viet Hoang Tran Duong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT3sg-hdUPlX9VamfzW6qUngDvYX0YJmdFBMNS5Q=s64","userId":"14828536041389094926"}},"outputId":"e2ce2338-046c-4217-d908-ccb027ce78c4"},"source":["output, text = get_query(\"machine learning\")\n","\n","print(\"Number of posts crawled:\", len(output))\n","print(\"Example of one crawled information:\", output[0])\n","print(\"\")\n","print(\"All the text of all posts (combined):\", text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of posts crawled: 10\n","Example of one crawled information: [86000, 'Machine Learning is Fun!', 'https://medium.com/search?q=machine%20learning', ['Machine Learning is Fun!', 'What is machine learning?', 'Two kinds of Machine Learning Algorithms', 'That’s cool, but does being able to estimate the price of a house really count as “learning”?', 'Let’s write that program!', 'Mind Blowage Time', 'What about that whole “try every number” bit in Step 3?', 'What else did you conveniently skip over?', 'Is machine learning magic?', 'How to learn more about Machine Learning']]\n","\n","All the text of all posts (combined): Machine Learning is Fun! What is machine learning? Two kinds of Machine Learning Algorithms That’s cool, but does being able to estimate the price of a house really count as “learning”? Let’s write that program! Mind Blowage Time What about that whole “try every number” bit in Step 3? What else did you conveniently skip over? Is machine learning magic? How to learn more about Machine Learning Sign in Adam Geitgey May 5, 2014·15 min read Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in 日本語, Português, Português (alternate), Türkçe, Français, 한국어 , العَرَبِيَّة‎‎, Español (México), Español (España), Polski, Italiano, 普通话, Русский, 한국어 , Tiếng Việt or فارسی. Giant update: I’ve written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now! Have you heard people talking about machine learning but only have a fuzzy idea of what that means? Are you tired of nodding your way through conversations with co-workers? Let’s change that! This guide is for anyone who is curious about machine learning but has no idea where to start. I imagine there are a lot of people who tried reading the wikipedia article, got frustrated and gave up wishing someone would just give them a high-level explanation. That’s what this is. The goal is be accessible to anyone — which means that there’s a lot of generalizations. But who cares? If this gets anyone more interested in ML, then mission accomplished. Machine learning is the idea that there are generic algorithms that can tell you something interesting about a set of data without you having to write any custom code specific to the problem. Instead of writing code, you feed data to the generic algorithm and it builds its own logic based on the data. For example, one kind of algorithm is a classification algorithm. It can put data into different groups. The same classification algorithm used to recognize handwritten numbers could also be used to classify emails into spam and not-spam without changing a line of code. It’s the same algorithm but it’s fed different training data so it comes up with different classification logic. “Machine learning” is an umbrella term covering lots of these kinds of generic algorithms. You can think of machine learning algorithms as falling into one of two main categories — supervised learning and unsupervised learning. The difference is simple, but really important. Let’s say you are a real estate agent. Your business is growing, so you hire a bunch of new trainee agents to help you out. But there’s a problem — you can glance at a house and have a pretty good idea of what a house is worth, but your trainees don’t have your experience so they don’t know how to price their houses. To help your trainees (and maybe free yourself up for a vacation), you decide to write a little app that can estimate the value of a house in your area based on it’s size, neighborhood, etc, and what similar houses have sold for. So you write down every time someone sells a house in your city for 3 months. For each house, you write down a bunch of details — number of bedrooms, size in square feet, neighborhood, etc. But most importantly, you write down the final sale price: Using that training data, we want to create a program that can estimate how much any other house in your area is worth: This is called supervised learning. You knew how much each house sold for, so in other words, you knew the answer to the problem and could work backwards from there to figure out the logic. To build your app, you feed your training data about each house into your machine learning algorithm. The algorithm is trying to figure out what kind of math needs to be done to make the numbers work out. This kind of like having the answer key to a math test with all the arithmetic symbols erased: From this, can you figure out what kind of math problems were on the test? You know you are supposed to “do something” with the numbers on the left to get each answer on the right. In supervised learning, you are letting the computer work out that relationship for you. And once you know what math was required to solve this specific set of problems, you could answer to any other problem of the same type! Let’s go back to our original example with the real estate agent. What if you didn’t know the sale price for each house? Even if all you know is the size, location, etc of each house, it turns out you can still do some really cool stuff. This is called unsupervised learning. This is kind of like someone giving you a list of numbers on a sheet of paper and saying “I don’t really know what these numbers mean but maybe you can figure out if there is a pattern or grouping or something — good luck!” So what could do with this data? For starters, you could have an algorithm that automatically identified different market segments in your data. Maybe you’d find out that home buyers in the neighborhood near the local college really like small houses with lots of bedrooms, but home buyers in the suburbs prefer 3-bedroom houses with lots of square footage. Knowing about these different kinds of customers could help direct your marketing efforts. Another cool thing you could do is automatically identify any outlier houses that were way different than everything else. Maybe those outlier houses are giant mansions and you can focus your best sales people on those areas because they have bigger commissions. Supervised learning is what we’ll focus on for the rest of this post, but that’s not because unsupervised learning is any less useful or interesting. In fact, unsupervised learning is becoming increasingly important as the algorithms get better because it can be used without having to label the data with the correct answer. Side note: There are lots of other types of machine learning algorithms. But this is a pretty good place to start. As a human, your brain can approach most any situation and learn how to deal with that situation without any explicit instructions. If you sell houses for a long time, you will instinctively have a “feel” for the right price for a house, the best way to market that house, the kind of client who would be interested, etc. The goal of Strong AI research is to be able to replicate this ability with computers. But current machine learning algorithms aren’t that good yet — they only work when focused a very specific, limited problem. Maybe a better definition for “learning” in this case is “figuring out an equation to solve a specific problem based on some example data”. Unfortunately “Machine Figuring out an equation to solve a specific problem based on some example data” isn’t really a great name. So we ended up with “Machine Learning” instead. Of course if you are reading this 50 years in the future and we’ve figured out the algorithm for Strong AI, then this whole post will all seem a little quaint. Maybe stop reading and go tell your robot servant to go make you a sandwich, future human. So, how would you write the program to estimate the value of a house like in our example above? Think about it for a second before you read further. If you didn’t know anything about machine learning, you’d probably try to write out some basic rules for estimating the price of a house like this: If you fiddle with this for hours and hours, you might end up with something that sort of works. But your program will never be perfect and it will be hard to maintain as prices change. Wouldn’t it be better if the computer could just figure out how to implement this function for you? Who cares what exactly the function does as long is it returns the correct number: One way to think about this problem is that the price is a delicious stew and the ingredients are the number of bedrooms, the square footage and the neighborhood. If you could just figure out how much each ingredient impacts the final price, maybe there’s an exact ratio of ingredients to stir in to make the final price. That would reduce your original function (with all those crazy if’s and else’s) down to something really simple like this: Notice the magic numbers in bold — .841231951398213, 1231.1231231, 2.3242341421, and 201.23432095. These are our weights. If we could just figure out the perfect weights to use that work for every house, our function could predict house prices! A dumb way to figure out the best weights would be something like this: Start with each weight set to 1.0: Run every house you know about through your function and see how far off the function is at guessing the correct price for each house: For example, if the first house really sold for $250,000, but your function guessed it sold for $178,000, you are off by $72,000 for that single house. Now add up the squared amount you are off for each house you have in your data set. Let’s say that you had 500 home sales in your data set and the square of how much your function was off for each house was a grand total of $86,123,373. That’s how “wrong” your function currently is. Now, take that sum total and divide it by 500 to get an average of how far off you are for each house. Call this average error amount the cost of your function. If you could get this cost to be zero by playing with the weights, your function would be perfect. It would mean that in every case, your function perfectly guessed the price of the house based on the input data. So that’s our goal — get this cost to be as low as possible by trying different weights. Repeat Step 2 over and over with every single possible combination of weights. Whichever combination of weights makes the cost closest to zero is what you use. When you find the weights that work, you’ve solved the problem! That’s pretty simple, right? Well think about what you just did. You took some data, you fed it through three generic, really simple steps, and you ended up with a function that can guess the price of any house in your area. Watch out, Zillow! But here’s a few more facts that will blow your mind: Pretty crazy, right? Ok, of course you can’t just try every combination of all possible weights to find the combo that works the best. That would literally take forever since you’d never run out of numbers to try. To avoid that, mathematicians have figured out lots of clever ways to quickly find good values for those weights without having to try very many. Here’s one way: First, write a simple equation that represents Step #2 above: Now let’s re-write exactly the same equation, but using a bunch of machine learning math jargon (that you can ignore for now): This equation represents how wrong our price estimating function is for the weights we currently have set. If we graph this cost equation for all possible values of our weights for number_of_bedrooms and sqft, we’d get a graph that might look something like this: In this graph, the lowest point in blue is where our cost is the lowest — thus our function is the least wrong. The highest points are where we are most wrong. So if we can find the weights that get us to the lowest point on this graph, we’ll have our answer! So we just need to adjust our weights so we are “walking down hill” on this graph towards the lowest point. If we keep making small adjustments to our weights that are always moving towards the lowest point, we’ll eventually get there without having to try too many different weights. If you remember anything from Calculus, you might remember that if you take the derivative of a function, it tells you the slope of the function’s tangent at any point. In other words, it tells us which way is downhill for any given point on our graph. We can use that knowledge to walk downhill. So if we calculate a partial derivative of our cost function with respect to each of our weights, then we can subtract that value from each weight. That will walk us one step closer to the bottom of the hill. Keep doing that and eventually we’ll reach the bottom of the hill and have the best possible values for our weights. (If that didn’t make sense, don’t worry and keep reading). That’s a high level summary of one way to find the best weights for your function called batch gradient descent. Don’t be afraid to dig deeper if you are interested on learning the details. When you use a machine learning library to solve a real problem, all of this will be done for you. But it’s still useful to have a good idea of what is happening. The three-step algorithm I described is called multivariate linear regression. You are estimating the equation for a line that fits through all of your house data points. Then you are using that equation to guess the sales price of houses you’ve never seen before based where that house would appear on your line. It’s a really powerful idea and you can solve “real” problems with it. But while the approach I showed you might work in simple cases, it won’t work in all cases. One reason is because house prices aren’t always simple enough to follow a continuous line. But luckily there are lots of ways to handle that. There are plenty of other machine learning algorithms that can handle non-linear data (like neural networks or SVMs with kernels). There are also ways to use linear regression more cleverly that allow for more complicated lines to be fit. In all cases, the same basic idea of needing to find the best weights still applies. Also, I ignored the idea of overfitting. It’s easy to come up with a set of weights that always works perfectly for predicting the prices of the houses in your original data set but never actually works for any new houses that weren’t in your original data set. But there are ways to deal with this (like regularization and using a cross-validation data set). Learning how to deal with this issue is a key part of learning how to apply machine learning successfully. In other words, while the basic concept is pretty simple, it takes some skill and experience to apply machine learning and get useful results. But it’s a skill that any developer can learn! Once you start seeing how easily machine learning techniques can be applied to problems that seem really hard (like handwriting recognition), you start to get the feeling that you could use machine learning to solve any problem and get an answer as long as you have enough data. Just feed in the data and watch the computer magically figure out the equation that fits the data! But it’s important to remember that machine learning only works if the problem is actually solvable with the data that you have. For example, if you build a model that predicts home prices based on the type of potted plants in each house, it’s never going to work. There just isn’t any kind of relationship between the potted plants in each house and the home’s sale price. So no matter how hard it tries, the computer can never deduce a relationship between the two. So remember, if a human expert couldn’t use the data to solve the problem manually, a computer probably won’t be able to either. Instead, focus on problems where a human could solve the problem, but where it would be great if a computer could solve it much more quickly. In my mind, the biggest problem with machine learning right now is that it mostly lives in the world of academia and commercial research groups. There isn’t a lot of easy to understand material out there for people who would like to get a broad understanding without actually becoming experts. But it’s getting a little better every day. If you want to try out what you’ve learned in this article, I made a course that walks you through every step of this article, including writing all the code. Give it a try! If you want to go deeper, Andrew Ng’s free Machine Learning class on Coursera is pretty amazing as a next step. I highly recommend it. It should be accessible to anyone who has a Comp. Sci. degree and who remembers a very minimal amount of math. Also, you can play around with tons of machine learning algorithms by downloading and installing SciKit-Learn. It’s a python framework that has “black box” versions of all the standard algorithms. If you liked this article, please consider signing up for my Machine Learning is Fun! Newsletter: Also, please check out the full-length course version of this article. It covers everything in this article in more detail, including writing the actual code in Python. You can get a free 30-day trial to watch the course if you sign up with this link. You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning. Now continue on to Machine Learning is Fun Part 2! Interested in computers and machine learning. Likes to write about it. 86K  261  86K  86K  261  Interested in computers and machine learning. Likes to write about it. About Help Legal Get the Medium app Cheat Sheets for AI, Neural Networks, Machine Learning, Deep Learning & Big Data Neural Networks >>> If you like this list, you can let me know here.<<< Neural Networks Graphs Machine Learning Overview Machine Learning: Scikit-learn algorithm Scikit-Learn MACHINE LEARNING : ALGORITHM CHEAT SHEET >>> If you like this list, you can let me know here. <<< Python for Data Science TensorFlow Keras Numpy Pandas Data Wrangling Data Wrangling with dplyr and tidyr Scipy Matplotlib Data Visualization PySpark Big-O About Stefan Resources Over the past few months, I have been collecting AI cheat sheets. From time to time I share them with friends and colleagues and recently I have been getting asked a lot, so I decided to organize and share the entire collection. To make things more interesting and give context, I added descriptions and/or excerpts for each major topic. This is the most complete list and the Big-O is at the very end, enjoy… >>> Update: We have recently redesigned these cheat sheets into a Super High Definition PDF. Check them out below: becominghuman.ai chatbotslife.com aijobsboard.com This machine learning cheat sheet will help you find the right estimator for the job which is the most difficult part. The flowchart will help you check the documentation and rough guide of each estimator that will help you to know more about the problems and how to solve it. >>> See Latest Jobs in AI, ML & BIG DATA <<< Scikit-learn (formerly scikits.learn) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. This machine learning cheat sheet from Microsoft Azure will help you choose the appropriate machine learning algorithms for your predictive analytics solution. First, the cheat sheet will asks you about the data nature and then suggests the best algorithm for the job. becominghuman.ai aijobsboard.com In May 2017 Google announced the second-generation of the TPU, as well as the availability of the TPUs in Google Compute Engine.[12] The second-generation TPUs deliver up to 180 teraflops of performance, and when organized into clusters of 64 TPUs provide up to 11.5 petaflops. becominghuman.ai In 2017, Google’s TensorFlow team decided to support Keras in TensorFlow’s core library. Chollet explained that Keras was conceived to be an interface rather than an end-to-end machine-learning framework. It presents a higher-level, more intuitive set of abstractions that make it easy to configure neural networks regardless of the backend scientific computing library. NumPy targets the CPython reference implementation of Python, which is a non-optimizing bytecode interpreter. Mathematical algorithms written for this version of Python often run much slower than compiled equivalents. NumPy address the slowness problem partly by providing multidimensional arrays and functions and operators that operate efficiently on arrays, requiring rewriting some code, mostly inner loops using NumPy. The name ‘Pandas’ is derived from the term “panel data”, an econometrics term for multidimensional structured data sets. The term “data wrangler” is starting to infiltrate pop culture. In the 2017 movie Kong: Skull Island, one of the characters, played by actor Marc Evan Jackson is introduced as “Steve Woodward, our data wrangler”. becominghuman.ai chatbotslife.com aijobsboard.com SciPy builds on the NumPy array object and is part of the NumPy stack which includes tools like Matplotlib, pandas and SymPy, and an expanding set of scientific computing libraries. This NumPy stack has similar users to other applications such as MATLAB, GNU Octave, and Scilab. The NumPy stack is also sometimes referred to as the SciPy stack.[3] matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK+. There is also a procedural “pylab” interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB, though its use is discouraged.[2] SciPy makes use of matplotlib. pyplot is a matplotlib module which provides a MATLAB-like interface.[6] matplotlib is designed to be as usable as MATLAB, with the ability to use Python, with the advantage that it is free. >>> If you like this list, you can let me know here. <<< aijobsboard.com becominghuman.ai becominghuman.ai Stefan is the founder of Chatbot’s Life, a Chatbot media and consulting firm. Chatbot’s Life has grown to over 150k views per month and has become the premium place to learn about Bots & AI online. Chatbot’s Life has also consulted many of the top Bot companies like Swelly, Instavest, OutBrain, NearGroup and a number of Enterprises. Big-O Algorithm Cheat Sheet: http://bigocheatsheet.com/ Bokeh Cheat Sheet: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Bokeh_Cheat_Sheet.pdf Data Science Cheat Sheet: https://www.datacamp.com/community/tutorials/python-data-science-cheat-sheet-basics Data Wrangling Cheat Sheet: https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf Data Wrangling: https://en.wikipedia.org/wiki/Data_wrangling Ggplot Cheat Sheet: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Keras Cheat Sheet: https://www.datacamp.com/community/blog/keras-cheat-sheet#gs.DRKeNMs Keras: https://en.wikipedia.org/wiki/Keras Machine Learning Cheat Sheet: https://ai.icymi.email/new-machinelearning-cheat-sheet-by-emily-barry-abdsc/ Machine Learning Cheat Sheet: https://docs.microsoft.com/en-in/azure/machine-learning/machine-learning-algorithm-cheat-sheet ML Cheat Sheet:: http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html Matplotlib Cheat Sheet: https://www.datacamp.com/community/blog/python-matplotlib-cheat-sheet#gs.uEKySpY Matpotlib: https://en.wikipedia.org/wiki/Matplotlib Neural Networks Cheat Sheet: http://www.asimovinstitute.org/neural-network-zoo/ Neural Networks Graph Cheat Sheet: http://www.asimovinstitute.org/blog/ Neural Networks: https://www.quora.com/Where-can-find-a-cheat-sheet-for-neural-network Numpy Cheat Sheet: https://www.datacamp.com/community/blog/python-numpy-cheat-sheet#gs.AK5ZBgE NumPy: https://en.wikipedia.org/wiki/NumPy Pandas Cheat Sheet: https://www.datacamp.com/community/blog/python-pandas-cheat-sheet#gs.oundfxM Pandas: https://en.wikipedia.org/wiki/Pandas_(software) Pandas Cheat Sheet: https://www.datacamp.com/community/blog/pandas-cheat-sheet-python#gs.HPFoRIc Pyspark Cheat Sheet: https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python#gs.L=J1zxQ Scikit Cheat Sheet: https://www.datacamp.com/community/blog/scikit-learn-cheat-sheet Scikit-learn: https://en.wikipedia.org/wiki/Scikit-learn Scikit-learn Cheat Sheet: http://peekaboo-vision.blogspot.com/2013/01/machine-learning-cheat-sheet-for-scikit.html Scipy Cheat Sheet: https://www.datacamp.com/community/blog/python-scipy-cheat-sheet#gs.JDSg3OI SciPy: https://en.wikipedia.org/wiki/SciPy TesorFlow Cheat Sheet: https://www.altoros.com/tensorflow-cheat-sheet.html Tensor Flow: https://en.wikipedia.org/wiki/TensorFlow Latest News, Info and Tutorials on Artificial Intelligence… 68K  150  Watch AI & Bot Conference for Free Take a look.  By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices. Check your inboxMedium sent you an email at  to complete your subscription. 68K claps 68K claps 150 responses Written by Founder of Chatbots Life. I help Companies Make More Money using Chatbots & AI and share my Insights along the way. Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. Written by Founder of Chatbots Life. I help Companies Make More Money using Chatbots & AI and share my Insights along the way. Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app Every single Machine Learning course on the internet, ranked by your reviews Now onto machine learning. How we picked courses to consider How we evaluated courses What is machine learning? What is a workflow? Do these courses cover deep learning? Recommended prerequisites Our pick for the best machine learning course is… A new Ivy League introduction with a brilliant professor A practical intro in Python & R from industry experts The competition Wrapping it Up A year and a half ago, I dropped out of one of the best computer science programs in Canada. I started creating my own data science master’s program using online resources. I realized that I could learn everything I needed through edX, Coursera, and Udacity instead. And I could learn it faster, more efficiently, and for a fraction of the cost. I’m almost finished now. I’ve taken many data science-related courses and audited portions of many more. I know the options out there, and what skills are needed for learners preparing for a data analyst or data scientist role. So I started creating a review-driven guide that recommends the best courses for each subject within data science. For the first guide in the series, I recommended a few coding classes for the beginner data scientist. Then it was statistics and probability classes. Then introductions to data science. Also, data visualization. For this guide, I spent a dozen hours trying to identify every online machine learning course offered as of May 2017, extracting key bits of information from their syllabi and reviews, and compiling their ratings. My end goal was to identify the three best courses available and present them to you, below. For this task, I turned to none other than the open source Class Central community, and its database of thousands of course ratings and reviews. Since 2011, Class Central founder Dhawal Shah has kept a closer eye on online courses than arguably anyone else in the world. Dhawal personally helped me assemble this list of resources. Each course must fit three criteria: We believe we covered every notable course that fits the above criteria. Since there are seemingly hundreds of courses on Udemy, we chose to consider the most-reviewed and highest-rated ones only. There’s always a chance that we missed something, though. So please let us know in the comments section if we left a good course out. We compiled average ratings and number of reviews from Class Central and other review sites to calculate a weighted average rating for each course. We read text reviews and used this feedback to supplement the numerical ratings. We made subjective syllabus judgment calls based on three factors: A popular definition originates from Arthur Samuel in 1959: machine learning is a subfield of computer science that gives “computers the ability to learn without being explicitly programmed.” In practice, this means developing computer programs that can make predictions based on data. Just as humans can learn from experience, so can computers, where data = experience. A machine learning workflow is the process required for carrying out a machine learning project. Though individual projects can differ, most workflows share several common tasks: problem evaluation, data exploration, data preprocessing, model training/testing/deployment, etc. Below you’ll find helpful visualization of these core steps: The ideal course introduces the entire process and provides interactive examples, assignments, and/or quizzes where students can perform each task themselves. First off, let’s define deep learning. Here is a succinct description: “Deep learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.” — Jason Brownlee from Machine Learning Mastery As would be expected, portions of some of the machine learning courses contain deep learning content. I chose not to include deep learning-only courses, however. If you are interested in deep learning specifically, we’ve got you covered with the following article: medium.freecodecamp.com My top three recommendations from that list would be: Several courses listed below ask students to have prior programming, calculus, linear algebra, and statistics experience. These prerequisites are understandable given that machine learning is an advanced discipline. Missing a few subjects? Good news! Some of this experience can be acquired through our recommendations in the first two articles (programming, statistics) of this Data Science Career Guide. Several top-ranked courses below also provide gentle calculus and linear algebra refreshers and highlight the aspects most relevant to machine learning for those less familiar. Stanford University’s Machine Learning on Coursera is the clear current winner in terms of ratings, reviews, and syllabus fit. Taught by the famous Andrew Ng, Google Brain founder and former chief scientist at Baidu, this was the class that sparked the founding of Coursera. It has a 4.7-star weighted average rating over 422 reviews. Released in 2011, it covers all aspects of the machine learning workflow. Though it has a smaller scope than the original Stanford class upon which it is based, it still manages to cover a large number of techniques and algorithms. The estimated timeline is eleven weeks, with two weeks dedicated to neural networks and deep learning. Free and paid options are available. Ng is a dynamic yet gentle instructor with a palpable experience. He inspires confidence, especially when sharing practical implementation tips and warnings about common pitfalls. A linear algebra refresher is provided and Ng highlights the aspects of calculus most relevant to machine learning. Evaluation is automatic and is done via multiple choice quizzes that follow each lesson and programming assignments. The assignments (there are eight of them) can be completed in MATLAB or Octave, which is an open-source version of MATLAB. Ng explains his language choice: In the past, I’ve tried to teach machine learning using a large variety of different programming languages including C++, Java, Python, NumPy, and also Octave … And what I’ve seen after having taught machine learning for almost a decade is that you learn much faster if you use Octave as your programming environment. Though Python and R are likely more compelling choices in 2017 with the increased popularity of those languages, reviewers note that that shouldn’t stop you from taking the course. A few prominent reviewers noted the following: Of longstanding renown in the MOOC world, Stanford’s machine learning course really is the definitive introduction to this topic. The course broadly covers all of the major areas of machine learning … Prof. Ng precedes each segment with a motivating discussion and examples. Andrew Ng is a gifted teacher and able to explain complicated subjects in a very intuitive and clear way, including the math behind all concepts. Highly recommended. The only problem I see with this course if that it sets the expectation bar very high for other courses. Columbia University’s Machine Learning is a relatively new offering that is part of their Artificial Intelligence MicroMasters on edX. Though it is newer and doesn’t have a large number of reviews, the ones that it does have are exceptionally strong. Professor John Paisley is noted as brilliant, clear, and clever. It has a 4.8-star weighted average rating over 10 reviews. The course also covers all aspects of the machine learning workflow and more algorithms than the above Stanford offering. Columbia’s is a more advanced introduction, with reviewers noting that students should be comfortable with the recommended prerequisites (calculus, linear algebra, statistics, probability, and coding). Quizzes (11), programming assignments (4), and a final exam are the modes of evaluation. Students can use either Python, Octave, or MATLAB to complete the assignments. The course’s total estimated timeline is eight to ten hours per week over twelve weeks. It is free with a verified certificate available for purchase. Below are a few of the aforementioned sparkling reviews: Over all my years of [being a] student I’ve come across professors who aren’t brilliant, professors who are brilliant but they don’t know how to explain the stuff clearly, and professors who are brilliant and know how explain the stuff clearly. Dr. Paisley belongs to the third group. This is a great course … The instructor’s language is precise and that is, to my mind, one of the strongest points of the course. The lectures are of high quality and the slides are great too. Dr. Paisley and his supervisor are … students of Michael Jordan, the father of machine learning. [Dr. Paisley] is the best ML professor at Columbia because of his ability to explain stuff clearly. Up to 240 students have selected his course this semester, the largest number among all professors [teaching] machine learning at Columbia. Machine Learning A-Z™ on Udemy is an impressively detailed offering that provides instruction in both Python and R, which is rare and can’t be said for any of the other top courses. It has a 4.5-star weighted average rating over 8,119 reviews, which makes it the most reviewed course of the ones considered. It covers the entire machine learning workflow and an almost ridiculous (in a good way) number of algorithms through 40.5 hours of on-demand video. The course takes a more applied approach and is lighter math-wise than the above two courses. Each section starts with an “intuition” video from Eremenko that summarizes the underlying theory of the concept being taught. de Ponteves then walks through implementation with separate videos for both Python and R. As a “bonus,” the course includes Python and R code templates for students to download and use on their own projects. There are quizzes and homework challenges, though these aren’t the strong points of the course. Eremenko and the SuperDataScience team are revered for their ability to “make the complex simple.” Also, the prerequisites listed are “just some high school mathematics,” so this course might be a better option for those daunted by the Stanford and Columbia offerings. A few prominent reviewers noted the following: The course is professionally produced, the sound quality is excellent, and the explanations are clear and concise … It’s an incredible value for your financial and time investment. It was spectacular to be able to follow the course in two different programming languages simultaneously. Kirill is one of the absolute best instructors on Udemy (if not the Internet) and I recommend taking any class he teaches. … This course has a ton of content, like a ton! Our #1 pick had a weighted average rating of 4.7 out of 5 stars over 422 reviews. Let’s look at the other alternatives, sorted by descending rating. A reminder that deep learning-only courses are not included in this guide — you can find those here. The Analytics Edge (Massachusetts Institute of Technology/edX): More focused on analytics in general, though it does cover several machine learning topics. Uses R. Strong narrative that leverages familiar real-world examples. Challenging. Ten to fifteen hours per week over twelve weeks. Free with a verified certificate available for purchase. It has a 4.9-star weighted average rating over 214 reviews. Python for Data Science and Machine Learning Bootcamp (Jose Portilla/Udemy): Has large chunks of machine learning content, but covers the whole data science process. More of a very detailed intro to Python. Amazing course, though not ideal for the scope of this guide. 21.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 3316 reviews. Data Science and Machine Learning Bootcamp with R (Jose Portilla/Udemy): The comments for Portilla’s above course apply here as well, except for R. 17.5 hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.6-star weighted average rating over 1317 reviews. Machine Learning Series (Lazy Programmer Inc./Udemy): Taught by a data scientist/big data engineer/full stack software engineer with an impressive resume, Lazy Programmer currently has a series of 16 machine learning-focused courses on Udemy. In total, the courses have 5000+ ratings and almost all of them have 4.6 stars. A useful course ordering is provided in each individual course’s description. Uses Python. Cost varies depending on Udemy discounts, which are frequent. Machine Learning (Georgia Tech/Udacity): A compilation of what was three separate courses: Supervised, Unsupervised and Reinforcement Learning. Part of Udacity’s Machine Learning Engineer Nanodegree and Georgia Tech’s Online Master’s Degree (OMS). Bite-sized videos, as is Udacity’s style. Friendly professors. Estimated timeline of four months. Free. It has a 4.56-star weighted average rating over 9 reviews. Implementing Predictive Analytics with Spark in Azure HDInsight (Microsoft/edX): Introduces the core concepts of machine learning and a variety of algorithms. Leverages several big data-friendly tools, including Apache Spark, Scala, and Hadoop. Uses both Python and R. Four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.5-star weighted average rating over 6 reviews. Data Science and Machine Learning with Python — Hands On! (Frank Kane/Udemy): Uses Python. Kane has nine years of experience at Amazon and IMDb. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 4139 reviews. Scala and Spark for Big Data and Machine Learning (Jose Portilla/Udemy): “Big data” focus, specifically on implementation in Scala and Spark. Ten hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.5-star weighted average rating over 607 reviews. Machine Learning Engineer Nanodegree (Udacity): Udacity’s flagship Machine Learning program, which features a best-in-class project review system and career support. The program is a compilation of several individual Udacity courses, which are free. Co-created by Kaggle. Estimated timeline of six months. Currently costs $199 USD per month with a 50% tuition refund available for those who graduate within 12 months. It has a 4.5-star weighted average rating over 2 reviews. Learning From Data (Introductory Machine Learning) (California Institute of Technology/edX): Enrollment is currently closed on edX, but is also available via CalTech’s independent platform (see below). It has a 4.49-star weighted average rating over 42 reviews. Learning From Data (Introductory Machine Learning) (Yaser Abu-Mostafa/California Institute of Technology): “A real Caltech course, not a watered-down version.” Reviews note it is excellent for understanding machine learning theory. The professor, Yaser Abu-Mostafa, is popular among students and also wrote the textbook upon which this course is based. Videos are taped lectures (with lectures slides picture-in-picture) uploaded to YouTube. Homework assignments are .pdf files. The course experience for online students isn’t as polished as the top three recommendations. It has a 4.43-star weighted average rating over 7 reviews. Mining Massive Datasets (Stanford University): Machine learning with a focus on “big data.” Introduces modern distributed file systems and MapReduce. Ten hours per week over seven weeks. Free. It has a 4.4-star weighted average rating over 30 reviews. AWS Machine Learning: A Complete Guide With Python (Chandra Lingam/Udemy): A unique focus on cloud-based machine learning and specifically Amazon Web Services. Uses Python. Nine hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 62 reviews. Introduction to Machine Learning & Face Detection in Python (Holczer Balazs/Udemy): Uses Python. Eight hours of on-demand video. Cost varies depending on Udemy discounts, which are frequent. It has a 4.4-star weighted average rating over 162 reviews. StatLearning: Statistical Learning (Stanford University): Based on the excellent textbook, “An Introduction to Statistical Learning, with Applications in R” and taught by the professors who wrote it. Reviewers note that the MOOC isn’t as good as the book, citing “thin” exercises and mediocre videos. Five hours per week over nine weeks. Free. It has a 4.35-star weighted average rating over 84 reviews. Machine Learning Specialization (University of Washington/Coursera): Great courses, but last two classes (including the capstone project) were canceled. Reviewers note that this series is more digestable (read: easier for those without strong technical backgrounds) than other top machine learning courses (e.g. Stanford’s or Caltech’s). Be aware that the series is incomplete with recommender systems, deep learning, and a summary missing. Free and paid options available. It has a 4.31-star weighted average rating over 80 reviews. From 0 to 1: Machine Learning, NLP & Python-Cut to the Chase (Loony Corn/Udemy): “A down-to-earth, shy but confident take on machine learning techniques.” Taught by four-person team with decades of industry experience together. Uses Python. Cost varies depending on Udemy discounts, which are frequent. It has a 4.2-star weighted average rating over 494 reviews. Principles of Machine Learning (Microsoft/edX): Uses R, Python, and Microsoft Azure Machine Learning. Part of the Microsoft Professional Program Certificate in Data Science. Three to four hours per week over six weeks. Free with a verified certificate available for purchase. It has a 4.09-star weighted average rating over 11 reviews. Big Data: Statistical Inference and Machine Learning (Queensland University of Technology/FutureLearn): A nice, brief exploratory machine learning course with a focus on big data. Covers a few tools like R, H2O Flow, and WEKA. Only three weeks in duration at a recommended two hours per week, but one reviewer noted that six hours per week would be more appropriate. Free and paid options available. It has a 4-star weighted average rating over 4 reviews. Genomic Data Science and Clustering (Bioinformatics V) (University of California, San Diego/Coursera): For those interested in the intersection of computer science and biology and how it represents an important frontier in modern science. Focuses on clustering and dimensionality reduction. Part of UCSD’s Bioinformatics Specialization. Free and paid options available. It has a 4-star weighted average rating over 3 reviews. Intro to Machine Learning (Udacity): Prioritizes topic breadth and practical tools (in Python) over depth and theory. The instructors, Sebastian Thrun and Katie Malone, make this class so fun. Consists of bite-sized videos and quizzes followed by a mini-project for each lesson. Currently part of Udacity’s Data Analyst Nanodegree. Estimated timeline of ten weeks. Free. It has a 3.95-star weighted average rating over 19 reviews. Machine Learning for Data Analysis (Wesleyan University/Coursera): A brief intro machine learning and a few select algorithms. Covers decision trees, random forests, lasso regression, and k-means clustering. Part of Wesleyan’s Data Analysis and Interpretation Specialization. Estimated timeline of four weeks. Free and paid options available. It has a 3.6-star weighted average rating over 5 reviews. Programming with Python for Data Science (Microsoft/edX): Produced by Microsoft in partnership with Coding Dojo. Uses Python. Eight hours per week over six weeks. Free and paid options available. It has a 3.46-star weighted average rating over 37 reviews. Machine Learning for Trading (Georgia Tech/Udacity): Focuses on applying probabilistic machine learning approaches to trading decisions. Uses Python. Part of Udacity’s Machine Learning Engineer Nanodegree and Georgia Tech’s Online Master’s Degree (OMS). Estimated timeline of four months. Free. It has a 3.29-star weighted average rating over 14 reviews. Practical Machine Learning (Johns Hopkins University/Coursera): A brief, practical introduction to a number of machine learning algorithms. Several one/two-star reviews expressing a variety of concerns. Part of JHU’s Data Science Specialization. Four to nine hours per week over four weeks. Free and paid options available. It has a 3.11-star weighted average rating over 37 reviews. Machine Learning for Data Science and Analytics (Columbia University/edX): Introduces a wide range of machine learning topics. Some passionate negative reviews with concerns including content choices, a lack of programming assignments, and uninspiring presentation. Seven to ten hours per week over five weeks. Free with a verified certificate available for purchase. It has a 2.74-star weighted average rating over 36 reviews. Recommender Systems Specialization (University of Minnesota/Coursera): Strong focus one specific type of machine learning — recommender systems. A four course specialization plus a capstone project, which is a case study. Taught using LensKit (an open-source toolkit for recommender systems). Free and paid options available. It has a 2-star weighted average rating over 2 reviews. Machine Learning With Big Data (University of California, San Diego/Coursera): Terrible reviews that highlight poor instruction and evaluation. Some noted it took them mere hours to complete the whole course. Part of UCSD’s Big Data Specialization. Free and paid options available. It has a 1.86-star weighted average rating over 14 reviews. Practical Predictive Analytics: Models and Methods (University of Washington/Coursera): A brief intro to core machine learning concepts. One reviewer noted that there was a lack of quizzes and that the assignments were not challenging. Part of UW’s Data Science at Scale Specialization. Six to eight hours per week over four weeks. Free and paid options available. It has a 1.75-star weighted average rating over 4 reviews. The following courses had one or no reviews as of May 2017. Machine Learning for Musicians and Artists (Goldsmiths, University of London/Kadenze): Unique. Students learn algorithms, software tools, and machine learning best practices to make sense of human gesture, musical audio, and other real-time data. Seven sessions in length. Audit (free) and premium ($10 USD per month) options available. It has one 5-star review. Applied Machine Learning in Python (University of Michigan/Coursera): Taught using Python and the scikit learn toolkit. Part of the Applied Data Science with Python Specialization. Scheduled to start May 29th. Free and paid options available. Applied Machine Learning (Microsoft/edX): Taught using various tools, including Python, R, and Microsoft Azure Machine Learning (note: Microsoft produces the course). Includes hands-on labs to reinforce the lecture content. Three to four hours per week over six weeks. Free with a verified certificate available for purchase. Machine Learning with Python (Big Data University): Taught using Python. Targeted towards beginners. Estimated completion time of four hours. Big Data University is affiliated with IBM. Free. Machine Learning with Apache SystemML (Big Data University): Taught using Apache SystemML, which is a declarative style language designed for large-scale machine learning. Estimated completion time of eight hours. Big Data University is affiliated with IBM. Free. Machine Learning for Data Science (University of California, San Diego/edX): Doesn’t launch until January 2018. Programming examples and assignments are in Python, using Jupyter notebooks. Eight hours per week over ten weeks. Free with a verified certificate available for purchase. Introduction to Analytics Modeling (Georgia Tech/edX): The course advertises R as its primary programming tool. Five to ten hours per week over ten weeks. Free with a verified certificate available for purchase. Predictive Analytics: Gaining Insights from Big Data (Queensland University of Technology/FutureLearn): Brief overview of a few algorithms. Uses Hewlett Packard Enterprise’s Vertica Analytics platform as an applied tool. Start date to be announced. Two hours per week over four weeks. Free with a Certificate of Achievement available for purchase. Introducción al Machine Learning (Universitas Telefónica/Miríada X): Taught in Spanish. An introduction to machine learning that covers supervised and unsupervised learning. A total of twenty estimated hours over four weeks. Machine Learning Path Step (Dataquest): Taught in Python using Dataquest’s interactive in-browser platform. Multiple guided projects and a “plus” project where you build your own machine learning system using your own data. Subscription required. The following six courses are offered by DataCamp. DataCamp’s hybrid teaching style leverages video and text-based instruction with lots of examples through an in-browser code editor. A subscription is required for full access to each course. Introduction to Machine Learning (DataCamp): Covers classification, regression, and clustering algorithms. Uses R. Fifteen videos and 81 exercises with an estimated timeline of six hours. Supervised Learning with scikit-learn (DataCamp): Uses Python and scikit-learn. Covers classification and regression algorithms. Seventeen videos and 54 exercises with an estimated timeline of four hours. Unsupervised Learning in R (DataCamp): Provides a basic introduction to clustering and dimensionality reduction in R. Sixteen videos and 49 exercises with an estimated timeline of four hours. Machine Learning Toolbox (DataCamp): Teaches the “big ideas” in machine learning. Uses R. 24 videos and 88 exercises with an estimated timeline of four hours. Machine Learning with the Experts: School Budgets (DataCamp): A case study from a machine learning competition on DrivenData. Involves building a model to automatically classify items in a school’s budget. DataCamp’s “Supervised Learning with scikit-learn” is a prerequisite. Fifteen videos and 51 exercises with an estimated timeline of four hours. Unsupervised Learning in Python (DataCamp): Covers a variety of unsupervised learning algorithms using Python, scikit-learn, and scipy. The course ends with students building a recommender system to recommend popular musical artists. Thirteen videos and 52 exercises with an estimated timeline of four hours. Machine Learning (Tom Mitchell/Carnegie Mellon University): Carnegie Mellon’s graduate introductory machine learning course. A prerequisite to their second graduate level course, “Statistical Machine Learning.” Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. A 2011 version of the course also exists. CMU is one of the best graduate schools for studying machine learning and has a whole department dedicated to ML. Free. Statistical Machine Learning (Larry Wasserman/Carnegie Mellon University): Likely the most advanced course in this guide. A follow-up to Carnegie Mellon’s Machine Learning course. Taped university lectures with practice problems, homework assignments, and a midterm (all with solutions) posted online. Free. Undergraduate Machine Learning (Nando de Freitas/University of British Columbia): An undergraduate machine learning course. Lectures are filmed and put on YouTube with the slides posted on the course website. The course assignments are posted as well (no solutions, though). de Freitas is now a full-time professor at the University of Oxford and receives praise for his teaching abilities in various forums. Graduate version available (see below). Machine Learning (Nando de Freitas/University of British Columbia): A graduate machine learning course. The comments in de Freitas’ undergraduate course (above) apply here as well. This is the fifth of a six-piece series that covers the best online courses for launching yourself into the data science field. We covered programming in the first article, statistics and probability in the second article, intros to data science in the third article, and data visualization in the fourth. medium.freecodecamp.com The final piece will be a summary of those articles, plus the best online courses for other key topics such as data wrangling, databases, and even software engineering. If you’re looking for a complete list of Data Science online courses, you can find them on Class Central’s Data Science and Big Data subject page. If you enjoyed reading this, check out some of Class Central’s other pieces: medium.freecodecamp.com medium.freecodecamp.com If you have suggestions for courses I missed, let me know in the responses! If you found this helpful, click the 💚 so more people will see it here on Medium. This is a condensed version of my original article published on Class Central, where I’ve included detailed course syllabi. This is no longer updated. 22K  72  22K claps 22K claps 72 responses Written by Curating the internet’s best data science program. This is no longer updated. Go to https://freecodecamp.org/news instead Written by Curating the internet’s best data science program. This is no longer updated. Go to https://freecodecamp.org/news instead Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app Essential Cheat Sheets for Machine Learning and Deep Learning Engineers Machine learning is complex. For newbies, starting to learn machine learning can be painful if they don’t have right resources to learn from. Most of the machine learning libraries are difficult to understand and learning curve can be a bit frustrating. I am creating a repository on Github(cheatsheets-ai) containing cheatsheets for different machine learning frameworks, gathered from different sources. Do visit the Github repository, also, contribute cheat sheets if you have any. Thanks. List of Cheatsheets: 1. Keras2. Numpy3. Pandas4. Scipy5. Matplotlib6. Scikit-learn7. Neural Networks Zoo8. ggplot29. PySpark10. R Studio11. Jupyter Notebook12. Dask 2. Numpy 3. Pandas 4. Scipy 5. Matplotlib 6. Scikit-learn 7. Neural Networks Zoo 8. ggplot2 9. PySpark 10. R Studio (dplyr and tidyr) 11. Jupyter Notebook 12. Dask Thank you for reading. If you want to get into contact, you can reach out to me at ahikailash1@gmail.com About Me: I am a Co-Founder of MateLabs, where we have built Mateverse, an ML Platform which enables everyone to easily build and train Machine Learning Models, without writing a single line of code. Note: Recently, I published a book on GANs titled “Generative Adversarial Networks Projects”, in which I covered most of the widely popular GAN architectures and their implementations. DCGAN, StackGAN, CycleGAN, Pix2pix, Age-cGAN, and 3D-GAN have been covered in details at the implementation level. Each architecture has a chapter dedicated to it. I have explained these networks in a very simple and descriptive language using Keras framework with Tensorflow backend. If you are working on GANs or planning to use GANs, give it a read and share your valuable feedback with me at ahikailash1@gmail.com www.amazon.com You can grab a copy of the book from http://www.amazon.com/Generative-Adversarial-Networks-Projects-next-generation/dp/1789136679https://www.amazon.in/Generative-Adversarial-Networks-Projects-next-generation/dp/1789136679?fbclid=IwAR0X2pDk4CTxn5GqWmBbKIgiB38WmFX-sqCpBNI8k9Z8I-KCQ7VWRpJXm7I https://www.packtpub.com/big-data-and-business-intelligence/generative-adversarial-networks-projects?fbclid=IwAR2OtU21faMFPM4suH_HJmy_DRQxOVwJZB0kz3ZiSbFb_MW7INYCqqV7U0c Triplebyte helps programmers find great companies to work at. They’ll go through a technical interview with you, match you with companies that are looking for people with your specific skill sets, and then fast track you through their hiring processes. Looking for a new job? Take Triplebyte’s quiz and get a job at top companies! Top publication dedicated to startups, venture capital, &… 24K  104  24K claps 24K claps 104 responses Written by Co-founder - Mate Labs | Co-founder - Raven Protocol | Author - Generative Adversarial Networks Projects | Democratizing Artificial Intelligence Top publication dedicated to startups, venture capital, & technology. Want to submit content editors@redwhale.co A Redwhale Publication→ https://redwhale.co Written by Co-founder - Mate Labs | Co-founder - Raven Protocol | Author - Generative Adversarial Networks Projects | Democratizing Artificial Intelligence Top publication dedicated to startups, venture capital, & technology. Want to submit content editors@redwhale.co A Redwhale Publication→ https://redwhale.co Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app 30 Amazing Machine Learning Projects for the Past Year (v.2018) For the past year, we’ve compared nearly 8,800 open source Machine Learning projects to pick Top 30 (0.3% chance). This is an extremely competitive list and it carefully picks the best open source Machine Learning libraries, datasets and apps published between January and December 2017. Mybridge AI evaluates the quality by considering popularity, engagement and recency. To give you an idea about the quality, the average number of Github stars is 3,558. Open source projects can be useful for data scientists. You can learn by reading the source code and build something on top of the existing projects. Give a plenty of time to play around with Machine Learning projects you may have missed for the past year. <Recommended Learning> A) Neural Networks Deep Learning A-Z™: Hands-On Artificial Neural Networks [68,745 recommends, 4.5/5 stars] B) TensorFlow Complete Guide to TensorFlow for Deep Learning with Python [17,834 recommends, 4.6/5 stars] <Others> A) Web hosting: Get free domain name for a year. For your ‘simple’ personal website or project site. (Click the numbers below. Credit given to the biggest contributor.) FastText: Library for fast text representation and classification. [11786 stars on Github]. Courtesy of Facebook Research ……….. [ Muse: Multilingual Unsupervised or Supervised word Embeddings, based on Fast Text. 695 stars on Github] Deep-photo-styletransfer: Code and data for paper “Deep Photo Style Transfer” [9747 stars on Github]. Courtesy of Fujun Luan, Ph.D. at Cornell University The world’s simplest facial recognition api for Python and the command line [8672 stars on Github]. Courtesy of Adam Geitgey Magenta: Music and Art Generation with Machine Intelligence [8113 stars on Github]. Sonnet: TensorFlow-based neural network library [5731 stars on Github]. Courtesy of Malcolm Reynolds at Deepmind deeplearn.js: A hardware-accelerated machine intelligence library for the web [5462 stars on Github]. Courtesy of Nikhil Thorat at Google Brain Fast Style Transfer in TensorFlow [4843 stars on Github]. Courtesy of Logan Engstrom at MIT Pysc2: StarCraft II Learning Environment [3683 stars on Github]. Courtesy of Timo Ewalds at DeepMind AirSim: Open source simulator based on Unreal Engine for autonomous vehicles from Microsoft AI & Research [3861 stars on Github]. Courtesy of Shital Shah at Microsoft Facets: Visualizations for machine learning datasets [3371 stars on Github]. Courtesy of Google Brain Style2Paints: AI colorization of images [3310 stars on Github]. Tensor2Tensor: A library for generalized sequence to sequence models — Google Research [3087 stars on Github]. Courtesy of Ryan Sepassi at Google Brain Image-to-image translation in PyTorch (e.g. horse2zebra, edges2cats, and more) [2847 stars on Github]. Courtesy of Jun-Yan Zhu, Ph.D at Berkeley Faiss: A library for efficient similarity search and clustering of dense vectors. [2629 stars on Github]. Courtesy of Facebook Research Fashion-mnist: A MNIST-like fashion product database [2780 stars on Github]. Courtesy of Han Xiao, Research Scientist Zalando Tech ParlAI: A framework for training and evaluating AI models on a variety of openly available dialog datasets [2578 stars on Github]. Courtesy of Alexander Miller at Facebook Research Fairseq: Facebook AI Research Sequence-to-Sequence Toolkit [2571 stars on Github]. Pyro: Deep universal probabilistic programming with Python and PyTorch [2387 stars on Github]. Courtesy of Uber AI Labs iGAN: Interactive Image Generation powered by GAN [2369 stars on Github]. Deep-image-prior: Image restoration with neural networks but without learning [2188 stars on Github]. Courtesy of Dmitry Ulyanov, Ph.D at Skoltech Face_classification: Real-time face detection and emotion/gender classification using fer2013/imdb datasets with a keras CNN model and openCV. [1967 stars on Github]. Speech-to-Text-WaveNet : End-to-end sentence level English speech recognition using DeepMind’s WaveNet and tensorflow [1961 stars on Github]. Courtesy of Namju Kim at Kakao Brain StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation [1954 stars on Github]. Courtesy of Yunjey Choi at Korea University Ml-agents: Unity Machine Learning Agents [1658 stars on Github]. Courtesy of Arthur Juliani, Deep Learning at Unity3D DeepVideoAnalytics: A distributed visual search and visual data analytics platform [1494 stars on Github]. Courtesy of Akshay Bhat, Ph.D at Cornell University OpenNMT: Open-Source Neural Machine Translation in Torch [1490 stars on Github]. Pix2pixHD: Synthesizing and manipulating 2048x1024 images with conditional GANs [1283 stars on Github]. Courtesy of Ming-Yu Liu at AI Research Scientist at Nvidia Horovod: Distributed training framework for TensorFlow. [1188 stars on Github]. Courtesy of Uber Engineering AI-Blocks: A powerful and intuitive WYSIWYG interface that allows anyone to create Machine Learning models [899 stars on Github]. Deep neural networks for voice conversion (voice style transfer) in Tensorflow [845 stars on Github]. Courtesy of Dabi Ahn, AI Research at Kakao Brain That’s it for Machine Learning Open Source of the Year. Visit our publication to find top posts for more programming skills. Read more and achieve more 24K  25  24K claps 24K claps 25 responses Written by We rank articles for professionals Read more and achieve more Written by We rank articles for professionals Read more and achieve more Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning How to use Machine Learning on a Very Complicated Problem Face Recognition — Step by Step Running this Yourself Sign in Adam Geitgey Jul 24, 2016·13 min read Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in 普通话, Русский, 한국어, Português, Tiếng Việt, فارسی or Italiano. Giant update: I’ve written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now! Have you noticed that Facebook has developed an uncanny ability to recognize your friends in your photographs? In the old days, Facebook used to make you to tag your friends in photos by clicking on them and typing in their name. Now as soon as you upload a photo, Facebook tags everyone for you like magic: This technology is called face recognition. Facebook’s algorithms are able to recognize your friends’ faces after they have been tagged only a few times. It’s pretty amazing technology — Facebook can recognize faces with 98% accuracy which is pretty much as good as humans can do! Let’s learn how modern face recognition works! But just recognizing your friends would be too easy. We can push this tech to the limit to solve a more challenging problem — telling Will Ferrell (famous actor) apart from Chad Smith (famous rock musician)! So far in Part 1, 2 and 3, we’ve used machine learning to solve isolated problems that have only one step — estimating the price of a house, generating new data based on existing data and telling if an image contains a certain object. All of those problems can be solved by choosing one machine learning algorithm, feeding in data, and getting the result. But face recognition is really a series of several related problems: As a human, your brain is wired to do all of this automatically and instantly. In fact, humans are too good at recognizing faces and end up seeing faces in everyday objects: Computers are not capable of this kind of high-level generalization (at least not yet…), so we have to teach them how to do each step in this process separately. We need to build a pipeline where we solve each step of face recognition separately and pass the result of the current step to the next step. In other words, we will chain together several machine learning algorithms: Let’s tackle this problem one step at a time. For each step, we’ll learn about a different machine learning algorithm. I’m not going to explain every single algorithm completely to keep this from turning into a book, but you’ll learn the main ideas behind each one and you’ll learn how you can build your own facial recognition system in Python using OpenFace and dlib. The first step in our pipeline is face detection. Obviously we need to locate the faces in a photograph before we can try to tell them apart! If you’ve used any camera in the last 10 years, you’ve probably seen face detection in action: Face detection is a great feature for cameras. When the camera can automatically pick out faces, it can make sure that all the faces are in focus before it takes the picture. But we’ll use it for a different purpose — finding the areas of the image we want to pass on to the next step in our pipeline. Face detection went mainstream in the early 2000's when Paul Viola and Michael Jones invented a way to detect faces that was fast enough to run on cheap cameras. However, much more reliable solutions exist now. We’re going to use a method invented in 2005 called Histogram of Oriented Gradients — or just HOG for short. To find faces in an image, we’ll start by making our image black and white because we don’t need color data to find faces: Then we’ll look at every single pixel in our image one at a time. For every single pixel, we want to look at the pixels that directly surrounding it: Our goal is to figure out how dark the current pixel is compared to the pixels directly surrounding it. Then we want to draw an arrow showing in which direction the image is getting darker: If you repeat that process for every single pixel in the image, you end up with every pixel being replaced by an arrow. These arrows are called gradients and they show the flow from light to dark across the entire image: This might seem like a random thing to do, but there’s a really good reason for replacing the pixels with gradients. If we analyze pixels directly, really dark images and really light images of the same person will have totally different pixel values. But by only considering the direction that brightness changes, both really dark images and really bright images will end up with the same exact representation. That makes the problem a lot easier to solve! But saving the gradient for every single pixel gives us way too much detail. We end up missing the forest for the trees. It would be better if we could just see the basic flow of lightness/darkness at a higher level so we could see the basic pattern of the image. To do this, we’ll break up the image into small squares of 16x16 pixels each. In each square, we’ll count up how many gradients point in each major direction (how many point up, point up-right, point right, etc…). Then we’ll replace that square in the image with the arrow directions that were the strongest. The end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way: To find faces in this HOG image, all we have to do is find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces: Using this technique, we can now easily find faces in any image: If you want to try this step out yourself using Python and dlib, here’s code showing how to generate and view HOG representations of images. Whew, we isolated the faces in our image. But now we have to deal with the problem that faces turned different directions look totally different to a computer: To account for this, we will try to warp each picture so that the eyes and lips are always in the sample place in the image. This will make it a lot easier for us to compare faces in the next steps. To do this, we are going to use an algorithm called face landmark estimation. There are lots of ways to do this, but we are going to use the approach invented in 2014 by Vahid Kazemi and Josephine Sullivan. The basic idea is we will come up with 68 specific points (called landmarks) that exist on every face — the top of the chin, the outside edge of each eye, the inner edge of each eyebrow, etc. Then we will train a machine learning algorithm to be able to find these 68 specific points on any face: Here’s the result of locating the 68 face landmarks on our test image: Now that we know were the eyes and mouth are, we’ll simply rotate, scale and shear the image so that the eyes and mouth are centered as best as possible. We won’t do any fancy 3d warps because that would introduce distortions into the image. We are only going to use basic image transformations like rotation and scale that preserve parallel lines (called affine transformations): Now no matter how the face is turned, we are able to center the eyes and mouth are in roughly the same position in the image. This will make our next step a lot more accurate. If you want to try this step out yourself using Python and dlib, here’s the code for finding face landmarks and here’s the code for transforming the image using those landmarks. Now we are to the meat of the problem — actually telling faces apart. This is where things get really interesting! The simplest approach to face recognition is to directly compare the unknown face we found in Step 2 with all the pictures we have of people that have already been tagged. When we find a previously tagged face that looks very similar to our unknown face, it must be the same person. Seems like a pretty good idea, right? There’s actually a huge problem with that approach. A site like Facebook with billions of users and a trillion photos can’t possibly loop through every previous-tagged face to compare it to every newly uploaded picture. That would take way too long. They need to be able to recognize faces in milliseconds, not hours. What we need is a way to extract a few basic measurements from each face. Then we could measure our unknown face the same way and find the known face with the closest measurements. For example, we might measure the size of each ear, the spacing between the eyes, the length of the nose, etc. If you’ve ever watched a bad crime show like CSI, you know what I am talking about: Ok, so which measurements should we collect from each face to build our known face database? Ear size? Nose length? Eye color? Something else? It turns out that the measurements that seem obvious to us humans (like eye color) don’t really make sense to a computer looking at individual pixels in an image. Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. Deep learning does a better job than humans at figuring out which parts of a face are important to measure. The solution is to train a Deep Convolutional Neural Network (just like we did in Part 3). But instead of training the network to recognize pictures objects like we did last time, we are going to train it to generate 128 measurements for each face. The training process works by looking at 3 face images at a time: Then the algorithm looks at the measurements it is currently generating for each of those three images. It then tweaks the neural network slightly so that it makes sure the measurements it generates for #1 and #2 are slightly closer while making sure the measurements for #2 and #3 are slightly further apart: After repeating this step millions of times for millions of images of thousands of different people, the neural network learns to reliably generate 128 measurements for each person. Any ten different pictures of the same person should give roughly the same measurements. Machine learning people call the 128 measurements of each face an embedding. The idea of reducing complicated raw data like a picture into a list of computer-generated numbers comes up a lot in machine learning (especially in language translation). The exact approach for faces we are using was invented in 2015 by researchers at Google but many similar approaches exist. This process of training a convolutional neural network to output face embeddings requires a lot of data and computer power. Even with an expensive NVidia Telsa video card, it takes about 24 hours of continuous training to get good accuracy. But once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Lucky for us, the fine folks at OpenFace already did this and they published several trained networks which we can directly use. Thanks Brandon Amos and team! So all we need to do ourselves is run our face images through their pre-trained network to get the 128 measurements for each face. Here’s the measurements for our test image: So what parts of the face are these 128 numbers measuring exactly? It turns out that we have no idea. It doesn’t really matter to us. All that we care is that the network generates nearly the same numbers when looking at two different pictures of the same person. If you want to try this step yourself, OpenFace provides a lua script that will generate embeddings all images in a folder and write them to a csv file. You run it like this. This last step is actually the easiest step in the whole process. All we have to do is find the person in our database of known people who has the closest measurements to our test image. You can do that by using any basic machine learning classification algorithm. No fancy deep learning tricks are needed. We’ll use a simple linear SVM classifier, but lots of classification algorithms could work. All we need to do is train a classifier that can take in the measurements from a new test image and tells which known person is the closest match. Running this classifier takes milliseconds. The result of the classifier is the name of the person! So let’s try out our system. First, I trained a classifier with the embeddings of about 20 pictures each of Will Ferrell, Chad Smith and Jimmy Falon: Then I ran the classifier on every frame of the famous youtube video of Will Ferrell and Chad Smith pretending to be each other on the Jimmy Fallon show: It works! And look how well it works for faces in different poses — even sideways faces! Let’s review the steps we followed: Now that you know how this all works, here’s instructions from start-to-finish of how run this entire face recognition pipeline on your own computer: UPDATE 4/9/2017: You can still follow the steps below to use OpenFace. However, I’ve released a new Python-based face recognition library called face_recognition that is much easier to install and use. So I’d recommend trying out face_recognition first instead of continuing below! I even put together a pre-configured virtual machine with face_recognition, OpenCV, TensorFlow and lots of other deep learning tools pre-installed. You can download and run it on your computer very easily. Give the virtual machine a shot if you don’t want to install all these libraries yourself! Original OpenFace instructions: If you liked this article, please consider signing up for my Machine Learning is Fun! newsletter: You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning. Now continue on to Machine Learning is Fun Part 5! Interested in computers and machine learning. Likes to write about it. 29K  189  29K  29K  189  Interested in computers and machine learning. Likes to write about it. About Help Legal Get the Medium app Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks Recognizing Objects with Deep Learning Starting Simple Tunnel Vision The Solution is Convolution How Convolution Works Building our Bird Classifier Testing our Network Where to go from here Sign in Adam Geitgey Jun 13, 2016·16 min read Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in 普通话, Русский, 한국어, Português, Tiếng Việt, فارسی or Italiano. Giant update: I’ve written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now! Are you tired of reading endless news stories about deep learning and not really knowing what that means? Let’s change that! This time, we are going to learn how to write programs that recognize objects in images using deep learning. In other words, we’re going to explain the black magic that allows Google Photos to search your photos based on what is in the picture: Just like Part 1 and Part 2, this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone — which means that there’s a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished! (If you haven’t already read part 1 and part 2, read them now!) You might have seen this famous xkcd comic before. The goof is based on the idea that any 3-year-old child can recognize a photo of a bird, but figuring out how to make a computer recognize objects has puzzled the very best computer scientists for over 50 years. In the last few years, we’ve finally found a good approach to object recognition using deep convolutional neural networks. That sounds like a a bunch of made up words from a William Gibson Sci-Fi novel, but the ideas are totally understandable if you break them down one by one. So let’s do it — let’s write a program that can recognize birds! Before we learn how to recognize pictures of birds, let’s learn how to recognize something much simpler — the handwritten number “8”. In Part 2, we learned about how neural networks can solve complex problems by chaining together lots of simple neurons. We created a small neural network to estimate the price of a house based on how many bedrooms it had, how big it was, and which neighborhood it was in: We also know that the idea of machine learning is that the same generic algorithms can be reused with different data to solve different problems. So let’s modify this same neural network to recognize handwritten text. But to make the job really simple, we’ll only try to recognize one letter — the numeral “8”. Machine learning only works when you have data — preferably a lot of data. So we need lots and lots of handwritten “8”s to get started. Luckily, researchers created the MNIST data set of handwritten numbers for this very purpose. MNIST provides 60,000 images of handwritten digits, each as an 18x18 image. Here are some “8”s from the data set: The neural network we made in Part 2 only took in a three numbers as the input (“3” bedrooms, “2000” sq. feet , etc.). But now we want to process images with our neural network. How in the world do we feed images into a neural network instead of just numbers? The answer is incredible simple. A neural network takes numbers as input. To a computer, an image is really just a grid of numbers that represent how dark each pixel is: To feed an image into our neural network, we simply treat the 18x18 pixel image as an array of 324 numbers: The handle 324 inputs, we’ll just enlarge our neural network to have 324 input nodes: Notice that our neural network also has two outputs now (instead of just one). The first output will predict the likelihood that the image is an “8” and thee second output will predict the likelihood it isn’t an “8”. By having a separate output for each type of object we want to recognize, we can use a neural network to classify objects into groups. Our neural network is a lot bigger than last time (324 inputs instead of 3!). But any modern computer can handle a neural network with a few hundred nodes without blinking. This would even work fine on your cell phone. All that’s left is to train the neural network with images of “8”s and not-“8\"s so it learns to tell them apart. When we feed in an “8”, we’ll tell it the probability the image is an “8” is 100% and the probability it’s not an “8” is 0%. Vice versa for the counter-example images. Here’s some of our training data: We can train this kind of neural network in a few minutes on a modern laptop. When it’s done, we’ll have a neural network that can recognize pictures of “8”s with a pretty high accuracy. Welcome to the world of (late 1980’s-era) image recognition! It’s really neat that simply feeding pixels into a neural network actually worked to build image recognition! Machine learning is magic! …right? Well, of course it’s not that simple. First, the good news is that our “8” recognizer really does work well on simple images where the letter is right in the middle of the image: But now the really bad news: Our “8” recognizer totally fails to work when the letter isn’t perfectly centered in the image. Just the slightest position change ruins everything: This is because our network only learned the pattern of a perfectly-centered “8”. It has absolutely no idea what an off-center “8” is. It knows exactly one pattern and one pattern only. That’s not very useful in the real world. Real world problems are never that clean and simple. So we need to figure out how to make our neural network work in cases where the “8” isn’t perfectly centered. We already created a really good program for finding an “8” centered in an image. What if we just scan all around the image for possible “8”s in smaller sections, one section at a time, until we find one? This approach called a sliding window. It’s the brute force solution. It works well in some limited cases, but it’s really inefficient. You have to check the same image over and over looking for objects of different sizes. We can do better than this! When we trained our network, we only showed it “8”s that were perfectly centered. What if we train it with more data, including “8”s in all different positions and sizes all around the image? We don’t even need to collect new training data. We can just write a script to generate new images with the “8”s in all kinds of different positions in the image: Using this technique, we can easily create an endless supply of training data. More data makes the problem harder for our neural network to solve, but we can compensate for that by making our network bigger and thus able to learn more complicated patterns. To make the network bigger, we just stack up layer upon layer of nodes: We call this a “deep neural network” because it has more layers than a traditional neural network. This idea has been around since the late 1960s. But until recently, training this large of a neural network was just too slow to be useful. But once we figured out how to use 3d graphics cards (which were designed to do matrix multiplication really fast) instead of normal computer processors, working with large neural networks suddenly became practical. In fact, the exact same NVIDIA GeForce GTX 1080 video card that you use to play Overwatch can be used to train neural networks incredibly quickly. But even though we can make our neural network really big and train it quickly with a 3d graphics card, that still isn’t going to get us all the way to a solution. We need to be smarter about how we process images into our neural network. Think about it. It doesn’t make sense to train a network to recognize an “8” at the top of a picture separately from training it to recognize an “8” at the bottom of a picture as if those were two totally different objects. There should be some way to make the neural network smart enough to know that an “8” anywhere in the picture is the same thing without all that extra training. Luckily… there is! As a human, you intuitively know that pictures have a hierarchy or conceptual structure. Consider this picture: As a human, you instantly recognize the hierarchy in this picture: Most importantly, we recognize the idea of a child no matter what surface the child is on. We don’t have to re-learn the idea of child for every possible surface it could appear on. But right now, our neural network can’t do this. It thinks that an “8” in a different part of the image is an entirely different thing. It doesn’t understand that moving an object around in the picture doesn’t make it something different. This means it has to re-learn the identify of each object in every possible position. That sucks. We need to give our neural network understanding of translation invariance — an “8” is an “8” no matter where in the picture it shows up. We’ll do this using a process called Convolution. The idea of convolution is inspired partly by computer science and partly by biology (i.e. mad scientists literally poking cat brains with weird probes to figure out how cats process images). Instead of feeding entire images into our neural network as one grid of numbers, we’re going to do something a lot smarter that takes advantage of the idea that an object is the same no matter where it appears in a picture. Here’s how it’s going to work, step by step — Similar to our sliding window search above, let’s pass a sliding window over the entire original image and save each result as a separate, tiny picture tile: By doing this, we turned our original image into 77 equally-sized tiny image tiles. Earlier, we fed a single image into a neural network to see if it was an “8”. We’ll do the exact same thing here, but we’ll do it for each individual image tile: However, there’s one big twist: We’ll keep the same neural network weights for every single tile in the same original image. In other words, we are treating every image tile equally. If something interesting appears in any given tile, we’ll mark that tile as interesting. We don’t want to lose track of the arrangement of the original tiles. So we save the result from processing each tile into a grid in the same arrangement as the original image. It looks like this: In other words, we’ve started with a large image and we ended with a slightly smaller array that records which sections of our original image were the most interesting. The result of Step 3 was an array that maps out which parts of the original image are the most interesting. But that array is still pretty big: To reduce the size of the array, we downsample it using an algorithm called max pooling. It sounds fancy, but it isn’t at all! We’ll just look at each 2x2 square of the array and keep the biggest number: The idea here is that if we found something interesting in any of the four input tiles that makes up each 2x2 grid square, we’ll just keep the most interesting bit. This reduces the size of our array while keeping the most important bits. So far, we’ve reduced a giant image down into a fairly small array. Guess what? That array is just a bunch of numbers, so we can use that small array as input into another neural network. This final neural network will decide if the image is or isn’t a match. To differentiate it from the convolution step, we call it a “fully connected” network. So from start to finish, our whole five-step pipeline looks like this: Our image processing pipeline is a series of steps: convolution, max-pooling, and finally a fully-connected network. When solving problems in the real world, these steps can be combined and stacked as many times as you want! You can have two, three or even ten convolution layers. You can throw in max pooling wherever you want to reduce the size of your data. The basic idea is to start with a large image and continually boil it down, step-by-step, until you finally have a single result. The more convolution steps you have, the more complicated features your network will be able to learn to recognize. For example, the first convolution step might learn to recognize sharp edges, the second convolution step might recognize beaks using it’s knowledge of sharp edges, the third step might recognize entire birds using it’s knowledge of beaks, etc. Here’s what a more realistic deep convolutional network (like you would find in a research paper) looks like: In this case, they start a 224 x 224 pixel image, apply convolution and max pooling twice, apply convolution 3 more times, apply max pooling and then have two fully-connected layers. The end result is that the image is classified into one of 1000 categories! So how do you know which steps you need to combine to make your image classifier work? Honestly, you have to answer this by doing a lot of experimentation and testing. You might have to train 100 networks before you find the optimal structure and parameters for the problem you are solving. Machine learning involves a lot of trial and error! Now finally we know enough to write a program that can decide if a picture is a bird or not. As always, we need some data to get started. The free CIFAR10 data set contains 6,000 pictures of birds and 52,000 pictures of things that are not birds. But to get even more data we’ll also add in the Caltech-UCSD Birds-200–2011 data set that has another 12,000 bird pics. Here’s a few of the birds from our combined data set: And here’s some of the 52,000 non-bird images: This data set will work fine for our purposes, but 72,000 low-res images is still pretty small for real-world applications. If you want Google-level performance, you need millions of large images. In machine learning, having more data is almost always more important that having better algorithms. Now you know why Google is so happy to offer you unlimited photo storage. They want your sweet, sweet data! To build our classifier, we’ll use TFLearn. TFlearn is a wrapper around Google’s TensorFlow deep learning library that exposes a simplified API. It makes building convolutional neural networks as easy as writing a few lines of code to define the layers of our network. Here’s the code to define and train the network: If you are training with a good video card with enough RAM (like an Nvidia GeForce GTX 980 Ti or better), this will be done in less than an hour. If you are training with a normal cpu, it might take a lot longer. As it trains, the accuracy will increase. After the first pass, I got 75.4% accuracy. After just 10 passes, it was already up to 91.7%. After 50 or so passes, it capped out around 95.5% accuracy and additional training didn’t help, so I stopped it there. Congrats! Our program can now recognize birds in images! Now that we have a trained neural network, we can use it! Here’s a simple script that takes in a single image file and predicts if it is a bird or not. But to really see how effective our network is, we need to test it with lots of images. The data set I created held back 15,000 images for validation. When I ran those 15,000 images through the network, it predicted the correct answer 95% of the time. That seems pretty good, right? Well… it depends! Our network claims to be 95% accurate. But the devil is in the details. That could mean all sorts of different things. For example, what if 5% of our training images were birds and the other 95% were not birds? A program that guessed “not a bird” every single time would be 95% accurate! But it would also be 100% useless. We need to look more closely at the numbers than just the overall accuracy. To judge how good a classification system really is, we need to look closely at how it failed, not just the percentage of the time that it failed. Instead of thinking about our predictions as “right” and “wrong”, let’s break them down into four separate categories — Using our validation set of 15,000 images, here’s how many times our predictions fell into each category: Why do we break our results down like this? Because not all mistakes are created equal. Imagine if we were writing a program to detect cancer from an MRI image. If we were detecting cancer, we’d rather have false positives than false negatives. False negatives would be the worse possible case — that’s when the program told someone they definitely didn’t have cancer but they actually did. Instead of just looking at overall accuracy, we calculate Precision and Recall metrics. Precision and Recall metrics give us a clearer picture of how well we did: This tells us that 97% of the time we guessed “Bird”, we were right! But it also tells us that we only found 90% of the actual birds in the data set. In other words, we might not find every bird but we are pretty sure about it when we do find one! Now that you know the basics of deep convolutional networks, you can try out some of the examples that come with tflearn to get your hands dirty with different neural network architectures. It even comes with built-in data sets so you don’t even have to find your own images. You also know enough now to start branching and learning about other areas of machine learning. Why not learn how to use algorithms to train computers how to play Atari games next? If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I’ll only email you when I have something new and awesome to share. It’s the best way to find out when I write more articles like this. You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning. Now continue on to Machine Learning is Fun Part 4, Part 5 and Part 6! Interested in computers and machine learning. Likes to write about it. 25K  188  25K  25K  188  Interested in computers and machine learning. Likes to write about it. About Help Legal Get the Medium app The Non-Technical Guide to Machine Learning & Artificial Intelligence How to Use This List Machine Learning and Artificial Intelligence News Sources Machine Intelligence Startups & Tools Enterprise Intelligence Enterprise Functions Autonomous Systems Agents Industries Healthcare Technology Stack People to Know in Machine Learning and AI Artificial Intelligence and ML Events I have a challenge for you. In a few seconds, I want you to stop reading this article, and follow the instructions below. Here you go: I hope my point is obvious. Machine learning and artificial intelligence (ML and AI) have seized Tech mindshare in a way few topics have in recent memory. A couple months ago I noticed people talking about artificial intelligence everywhere I looked. According to AI experts, everything from our jobs, to the wars we wage, to the food we eat, to the beer we drink, to the software we write will be affected. Not being one to enjoy surprises, I decided to spend my free time learning as much about the space (and what the future holds) as possible. ML and AI are having a huge impact on our lives, and their roles are only increasing. The better informed you are, the better prepared you’ll be to handle these changes as they happen. Rather than sit here and pretend I know everything there is to know about ML and AI, I’m going to hand you the resources I use to educate myself. I hope you will use them too. Join 30,000+ people who read the weekly 🤖Machine Learnings🤖 newsletter to understand how AI will impact the way we work and live. There is already a ton of technical content being produced about artificial intelligence and machine learning. This list is a primer for non-technical people who want to understand what machine learning makes possible. To develop a deep understanding of the space, reading won’t be enough. You need to: have an understanding of the entire landscape, spot and use ML-enabled products in your daily life (Spotify recommendations), discuss artificial intelligence more regularly, and make friends with people who know more than you do about AI and ML. Startups: I’ve included links to the websites and apps of 307+ machine intelligence companies and tools. Alongside Mark Philpot, Avi Eisenberger, and Samiur Rahman, I’m building Journal — a product that uses machine learning to help people remember all the information they’ve come across that will be useful in the future. News: For starters, I’ve included a link to a weekly artificial intelligence email that Avi Eisenberger and I curate (🤖Subscribe to Machine Learnings🤖). Start here if you want to develop a better understanding of the space, but don’t have the time to actively hunt for machine learning and artificial intelligence news. People: Here’s a good place to jump into the conversation. I’ve provided links to Twitter accounts (and LinkedIn profiles and personal websites in their absence) of the founders, investors, writers, operators and researchers who work in and around the machine learning space. Events: If you enjoy getting out from behind your computer, and want to meet awesome people who are interested in artificial intelligence in real life, there is one place that’s best to do that, more on my favorite place below. subscribe.machinelearnings.co algorithmtips.org designmind.frogdesign.com www.bradfordcross.com machinelearnings.co hbr.org www.oreilly.com www.recode.net a16z.com www.tesla.com multithreaded.stitchfix.com www.economist.com www.technologyreview.com medium.com arxiv.org www.bloomberg.com www.csis.org techcrunch.com www.popsci.com Science news and science articles from New Scientistwww.newscientist.com www.wired.com research.googleblog.com Innovation and Startups at the Intersection of Technology and www.techemergence.com aiinvestor.com bigmedium.com www.nytimes.com www.technologyreview.com qz.com www.theatlantic.com www.economist.com All Tech Consideredwww.npr.org The future is wonderful, the future is terrifying.motherboard.vice.com deepmind.com One of the major promises of AI is freeing people from mindless tasks, so they can do more meaningful work. When we’re not writing posts about AI, we’re building Journal to help you see and search your private work information, all in one place. Get Early Access Shivon Zilis and James Cham, who invest in machine learning-related companies for Bloomberg Beta, recently created a machine intelligence market landscape. Below, you can find links to the 317+ companies in the landscape (and a few more), and play around with some apps that are applying machine learning in interesting ways. AlgocianCaptricityClarifaiCorticaDeepomaticDeepVisionNetraOrbital InsightPlanetSpaceknow CapioClover IntelligenceGridspaceMindMeldMobvoiNexidiaPop Up ArchiveQuirious.aiTalkIQTwilio AlluviumC3 IoTPlanet OSMaanaKONUXImubitGE PredixThingWorxUptakeSentenaiPreferred Networks AlationArimoCycorp Deckard.aiDigital ReasoningIBM WatsonKyndiDatabricksSapho BottlenoseCB InsightsDataFoxEnigma Intelligent LayerMattermarkPredataPremiseQuidTracxn ActionIQClarabridgeEloquent LabsKasistoPreactWise.ioZendesk 6senseAppZenAvisoClariCollective[i]FusemachinesInsideSalesSalesforce EinsteinZensight AirPRBrightFunnelCogniCorLatticeLiftIgniterMintigomsg.aiPersadoRadiusRetention Science CylanceDarktraceDeep InstinctDemistoDrawbridge NetworksGraphistryLeapYearSentinelOneSignalSenseZimperium EnteloAlgorithmia HiQHireVueSpringRoleTextioUnitiveWade & Wendy AdasWorksAuro RoboticsDrive.aiGoogleMobileyenuTonomyTeslaUberZoox AirwareDJIDroneDeployLilyPilot AI LabsShield AISkycatchSkydio Clearpath RoboticsFetch RoboticsHarvest AutomationJaybridgeRoboticsKindred AIOsaroRethink Robotics Amazon AlexaApple SiriFacebook MGoogle Now/AlloMicrosoft CortanaReplika Journal Alien LabsButter.aiClara LabsSlackSudoTallax.aiZoom.ai Abundant RoboticsAgriDataBlue River TechnologyDescartes LabsMavrxPivot BioTerrAvionTrace GenomicsTuleUDIO AltSchoolContent Technologies (CTI)CourseraGradescopeKnewtonVolley AlphaSenseBloombergCerebellum CapitalDataminriSentiumKenshoQuandlSentient BeagleBlue J LegalLegal RobotRavel LawROSS IntelligenceSeal AcertaClearMetalMarbleNAUTOPitStopPretecktRoutific CalcularioCitrineEigen InnovationsGinkgo BioworksNanotronicsSight MachineZymergen AffirmBettermentEarnestLendoMiradorTala (a InVenture)WealthfrontZestFinance AtomwiseCareSkoreDeep6 AnalyticsIBM Watson HealthNumerate MedicalOncorapulseDataSentrianZephyr Health DreamUp Vision 3ScanArterysBay LabsButterfly NetworkEnliticGoogle DeepMindImagia AtomwiseColor GenomicsDeep GenomicsGrailiCarbonXLuministNumerateRecursion PharmaceuticalsVerilyWhole Biome AutomatHowdyKasistoKITT.AIMaluubaOctane AIOpenAI GymSemantic Machines AyasdiBigMLDataikuDataRobotDomino Data LabKaggleRapidMinerSeldonObviously AI SparkBeyondYhatYseop BonsaiScaleContextRelevantCycorpDatacraticdeepsense.ioGeometric IntelligenceH2O.aiHyperScienceLoop AI Labsminds.aiNara LogicsReactiveScaled InferenceSkymindSparkCognition AgoloAYLIENCortical.ioLexalyticsLoop AI LabsLuminosoMonkeyLearnNarrative SciencespaCy AnOdotBonsai Deckard.aiFuzzy.aiHyperoptKiteLayer 6 AILobe.aiRainforestQASignifAISigOpt Amazon Mechanical TurkCrowdAICrowdFlowerDatalogueDataSiftdiffbotEnigmaImport.ioPaxataTrifactaWorkFusion Amazon DSSTNEApache SparkAzure MLBaiduCaffe ChainerDeepLearning4jH2O.aiKerasMicrosoft CNTKMicrosoft DMTKMLlibMXNetNervana NeonPaddlePaddlescikit-learnTensorFlowTheanoTorch7Weka 1026 LabsCadenceCirrascaleGoogle TPUIntel (Nervana)IsoclineKNUPATHNVIDIA DGX-1/Titan XQualcomm TenstorrentTensilica CogitaiKimeraKnogginNNAISENSENumentaOpenAIVicarious Andrew Ng Chief Scientist of Baidu; Chairman and Co-Founder of Coursera; Stanford CS faculty. Sam Altman President, YC Group, OpenAI co-chairman. Harry Shum EVP, Microsoft AI and Research. Geoffrey Hinton The godfather of deep learning. Samiur Rahman CEO of Journal. Former Data Engineering Lead at Mattermark. Jeff Dean Google Senior Fellow at Google, Inc. Co-founder and leader of Google’s deep learning research and engineering team. Eric Horvitz Technical Fellow at Microsoft Research Denny Britz Deep Learning at Google Brain. Tom Mitchell Computer scientist and E. Fredkin University Professor at the Carnegie Mellon University. Chris Dixon General Partner at Andreessen Horowitz. Hilary Mason Founder at FastForwardLabs. Data Scientist in Residence at Accel. Elon Musk Tesla Motors, SpaceX, SolarCity, PayPal & OpenAI. Kirk Borne The Principal Data Scientist at Booz Allen, PhD Astrophysicist. Peter Skomoroch Co-Founder & CEO SkipFlag. Previously Principal Data Scientist at LinkedIn, Engineer at AOL. Paul Barba Chief Scientist at Lexalytics. Andrej Karpathy Research scientist at OpenAI. Previously CS PhD student at Stanford. Monica Rogati Former VP of Data Jawbone & LinkedIn data scientist. Xavier Amatriain Leading Engineering at Quora. Netflix alumni. Mike Gualtieri Forrester VP & Principal Analyst. Fei-Fei Li Professor of Computer Science, Stanford University, Director of Stanford AI Lab. David Silver Royal Society University Research Fellow. Nando de Freitas Professor of Computer ScienceFellow, Linacre College. Roberto Cipolla Department of Engineering, University of Cambridge. Gabe Brostow Associate Professor in Computer Science at London’s Global University. Arthur Gretton Associate Professor with the Gatsby Computational Neuroscience Unit. Ingmar Posner University Lecturer in Engineering Science at the University of Oxford. Pieter Abbeel Associate Professor, UC Berkeley, EECS. Berkeley Artificial Intelligence Research (BAIR) laboratory. UC Berkeley Center for Human Compatible AI. Co-Founder Gradescope. Josh Wills Slack Data Engineering and Apache Crunch committer. Noah Weiss Head of Search, Learning, & Intelligence at Slack in NYC. Former SVP of Product at foursquare + Google PM on structured search. Michael E. Driscoll Founder, CEO Metamarkets. Investor at Data Collective Drew Conway Founder and CEO of Alluvium. Sean Taylor Facebook Data Science Team Demis Hassabis Co-Founder & CEO, DeepMind. Randy Olson Senior Data Scientist at Penn Institute for Biomedical Informatics. Shivon Zilis Partner at Bloomberg Beta where she focuses on machine intelligence companies. Adam Gibson Founder of Skymind. Alexandra Suich Technology reporter for The Economist. Anthony Goldblum Co-founder and CEO of Kaggle. Avi Goldfarb Professor at Rotman, University of Toronto and the Chief Data Scientist at Creative Destruction Lab. Ben Lorica Chief Data Scientist of O'Reilly Media, and Program Director of O’Reilly Strata & O’ReillyAI conferences. Ben hosts the O’Reilly Data Show Podcast too. Chris Nicholson Co-founder Deeplearning4j & Skymind. Previous to that, Chris worked at The New York Times. Doug Fulop Product manager at Kindred.ai. Dror Berman Founder, Innovation Endeavors. Dylan Tweney Founder of @TweneyMedia, former EIC @venturebeat, ex-@WIRED, publisher of @tinywords. Gary Kazantsev R&D Machine Learning at Bloomberg LP. Gideon Mann Head of Data Science / CTO Office at Bloomberg LP. Gordon Ritter Cloud investor at Emergence Capital, cloud entrepreneur. Jack Clark Strategy and Communications Director OpenAI. Past: @business World’s Only Neural Net Reporter. @theregister Distributed Systems Reporter. Federico Pascual COO & Co-Founder, MonkeyLearn. Matt Turck VC at FirstMark Capital and the organizer of Data Driven NYC and Hardwired NYC. Nick Adams Data Scientist, Berkeley Institute for Data Science. Roger Magoulas Research Director, O’Reilly Media. Sean Gourley Former CEO, Quid. Shruti Gandhi Array.VC, previously at True & Samsung Ventures. Steve Jurvetson Partner at Draper Fisher Jurvetson. Vijay Sundaram Venture Capitalist Innovation Endeavors, Tinkerer Polkadot Labs. Zavain Dar VC Lux Capital, Lecturer Stanford University, Moneyball Philadelphia 76ers. Yann Lecun Director of AI Research, Facebook. Founding Director of the NYU Center for Data Science Sarah Catanzaro Investor @canvasvc; former Data Lead @Mattermark. Paul Hsiao AI + Marketplace Investor @CanvasVC. Dennis Mortensen CEO and founder of x.ai. Pedro Domingos. Professor of computer science at UW and author of ‘The Master Algorithm’. Ryan Dawidjan.Product Team, Clarifai. Russ Salakhutdinov. Professor at Carnegie Mellon University. Aerin Kim. Unapologetic abuser of the machine BYOR. Lucy Wang. Data Scientist, VC Greycroft Partners, MS NYU Center for Data Science. Miles Brundage. AI Policy Research Fellow, Future of Humanity Institute, Oxford. PhD candidate, Human and Social Dimensions of Science and Technology, ASU. Rizwan Habib. Founder & Organizer, New York Artificial Intelligence (NYAI). Andreas Mueller. Lecturer at the Columbia Data Science Institute. Roger Chen Program Chair, O’Reilly Artificial Intelligence Conference. Jason Yosinski Machine Learning Scientist at Geometric Intelligence. Soumith Chintala Artificial Intelligence Research Engineer at Facebook. Suchi Saria Assistant Professor at Johns Hopkins University. Expertise in machine learning, streaming data and health informatics. Rob McInerney Founder and CEO of Intelligent Layer. PhD in Machine Learning at Oxford University. Peter Norvig Research Director at Google. Sebastian Thrun CEO of Udacity and part-time research professor of computer science at Stanford. Zoubin Ghahramani Researcher and Professor of Information Engineering at the University of Cambridge. Daphne Koller Professor of Computer Science at Stanford, Co-Founder of Coursera. Amir Banifatemi Leads and provides thought leadership for the Global IBM Watson Artificial Intelligence Xprize prize. Frank Chen Partner at Andreessen Horowitz. Lili Cheng General Manager, Fuse Labs, Microsoft Research. Shane Legg Chief Scientist and Co-Founder, Deepmind. Mustafa Suleyman Co-founder of Deepmind. Nathan Benaich Investor at Playfair Capital. I’ve written previously about my (lack of) love for networking events. NYAI has managed to rise above the noise and organize ~1,500 researchers, students, founders, and investors who are genuinely awesome people. The impacts that machine learning and AI have on our world will not be felt in isolation. These changes will affect all of us. NYAI’s monthly events are the best way, that I’ve found so far, to learn about, collaborate on and network over emerging trends in artificial intelligence. Understand how machine learning and artificial intelligence… 7.6K  56  7.6K claps 7.6K claps 56 responses Written by Co-founder, Journal Understand how machine learning and artificial intelligence will change your work & life. Written by Co-founder, Journal Understand how machine learning and artificial intelligence will change your work & life. Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface. Learn more Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox. Explore If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic. Write on Medium About Help Legal Get the Medium app Machine Learning is Fun! Part 2 Making Smarter Guesses What is a Neural Network? Giving our Neural Network a Memory What’s a single letter good for? Generating a story Making Mario without actually Making Mario Training Our Model Toys vs. Real World Applications Further Reading Sign in Adam Geitgey Jan 3, 2016·15 min read Update: This article is part of a series. Check out the full series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7 and Part 8! You can also read this article in Italiano, Español, Français, Türkçe, Русский, 한국어 Português, فارسی, Tiếng Việt or 普通话. Giant update: I’ve written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now! In Part 1, we said that Machine Learning is using generic algorithms to tell you something interesting about your data without writing any code specific to the problem you are solving. (If you haven’t already read part 1, read it now!). This time, we are going to see one of these generic algorithms do something really cool — create video game levels that look like they were made by humans. We’ll build a neural network, feed it existing Super Mario levels and watch new ones pop out! Just like Part 1, this guide is for anyone who is curious about machine learning but has no idea where to start. The goal is be accessible to anyone — which means that there’s a lot of generalizations and we skip lots of details. But who cares? If this gets anyone more interested in ML, then mission accomplished. Back in Part 1, we created a simple algorithm that estimated the value of a house based on its attributes. Given data about a house like this: We ended up with this simple estimation function: In other words, we estimated the value of the house by multiplying each of its attributes by a weight. Then we just added those numbers up to get the house’s value. Instead of using code, let’s represent that same function as a simple diagram: However this algorithm only works for simple problems where the result has a linear relationship with the input. What if the truth behind house prices isn’t so simple? For example, maybe the neighborhood matters a lot for big houses and small houses but doesn’t matter at all for medium-sized houses. How could we capture that kind of complicated detail in our model? To be more clever, we could run this algorithm multiple times with different of weights that each capture different edge cases: Now we have four different price estimates. Let’s combine those four price estimates into one final estimate. We’ll run them through the same algorithm again (but using another set of weights)! Our new Super Answer combines the estimates from our four different attempts to solve the problem. Because of this, it can model more cases than we could capture in one simple model. Let’s combine our four attempts to guess into one big diagram: This is a neural network! Each node knows how to take in a set of inputs, apply weights to them, and calculate an output value. By chaining together lots of these nodes, we can model complex functions. There’s a lot that I’m skipping over to keep this brief (including feature scaling and the activation function), but the most important part is that these basic ideas click: It’s just like LEGO! We can’t model much with one single LEGO block, but we can model anything if we have enough basic LEGO blocks to stick together: The neural network we’ve seen always returns the same answer when you give it the same inputs. It has no memory. In programming terms, it’s a stateless algorithm. In many cases (like estimating the price of house), that’s exactly what you want. But the one thing this kind of model can’t do is respond to patterns in data over time. Imagine I handed you a keyboard and asked you to write a story. But before you start, my job is to guess the very first letter that you will type. What letter should I guess? I can use my knowledge of English to increase my odds of guessing the right letter. For example, you will probably type a letter that is common at the beginning of words. If I looked at stories you wrote in the past, I could narrow it down further based on the words you usually use at the beginning of your stories. Once I had all that data, I could use it to build a neural network to model how likely it is that you would start with any given letter. Our model might look like this: But let’s make the problem harder. Let’s say I need to guess the next letter you are going to type at any point in your story. This is a much more interesting problem. Let’s use the first few words of Ernest Hemingway’s The Sun Also Rises as an example: Robert Cohn was once middleweight boxi What letter is going to come next? You probably guessed ’n’ — the word is probably going to be boxing. We know this based on the letters we’ve already seen in the sentence and our knowledge of common words in English. Also, the word ‘middleweight’ gives us an extra clue that we are talking about boxing. In other words, it’s easy to guess the next letter if we take into account the sequence of letters that came right before it and combine that with our knowledge of the rules of English. To solve this problem with a neural network, we need to add state to our model. Each time we ask our neural network for an answer, we also save a set of our intermediate calculations and re-use them the next time as part of our input. That way, our model will adjust its predictions based on the input that it has seen recently. Keeping track of state in our model makes it possible to not just predict the most likely first letter in the story, but to predict the most likely next letter given all previous letters. This is the basic idea of a Recurrent Neural Network. We are updating the network each time we use it. This allows it to update its predictions based on what it saw most recently. It can even model patterns over time as long as we give it enough of a memory. Predicting the next letter in a story might seem pretty useless. What’s the point? One cool use might be auto-predict for a mobile phone keyboard: But what if we took this idea to the extreme? What if we asked the model to predict the next most likely character over and over — forever? We’d be asking it to write a complete story for us! We saw how we could guess the next letter in Hemingway’s sentence. Let’s try generating a whole story in the style of Hemingway. To do this, we are going to use the Recurrent Neural Network implementation that Andrej Karpathy wrote. Andrej is a Deep-Learning researcher at Stanford and he wrote an excellent introduction to generating text with RNNs, You can view all the code for the model on github. We’ll create our model from the complete text of The Sun Also Rises — 362,239 characters using 84 unique letters (including punctuation, uppercase/lowercase, etc). This data set is actually really small compared to typical real-world applications. To generate a really good model of Hemingway’s style, it would be much better to have at several times as much sample text. But this is good enough to play around with as an example. As we just start to train the RNN, it’s not very good at predicting letters. Here’s what it generates after a 100 loops of training: hjCTCnhoofeoxelif edElobe negnk e iohehasenoldndAmdaI ayio pe e h’e btentmuhgehi bcgdltt. gey heho grpiahe.Ddelnss.eelaishaner” cot AAfhB ht ltnyehbih a”on bhnte ectrsnae abeahngyamo k ns aeo?cdse nh a taei.rairrhelardr er deffijha You can see that it has figured out that sometimes words have spaces between them, but that’s about it. After about 1000 iterations, things are looking more promising: hing soor ither. And the caraos, and the crowebel for figttier and ale the room of me? Streat was not to him Bill-stook of the momansbed mig out ust on the bull, out here. I been somsinick stalling that aid. “Hon’t me and acrained on .Hw’s don’t you for the roed,” In’s pair.” “Alough marith him.” The model has started to identify the patterns in basic sentence structure. It’s adding periods at the ends of sentences and even quoting dialog. A few words are recognizable, but there’s also still a lot of nonsense. But after several thousand more training iterations, it looks pretty good: He went over to the gate of the café. It was like a country bed. “Do you know it’s been me.” “Damned us,” Bill said. “I was dangerous,” I said. “You were she did it and think I would a fine cape you,” I said. “I can’t look strange in the cab.” “You know I was this is though,” Brett said. “It’s a fights no matter?” “It makes to do it.” “You make it?” “Sit down,” I said. “I wish I wasn’t do a little with the man.” “You found it.” “I don’t know.” “You see, I’m sorry of chatches,” Bill said. “You think it’s a friend off back and make you really drunk.” At this point, the algorithm has captured the basic pattern of Hemingway’s short, direct dialog. A few sentences even sort of make sense. Compare that with some real text from the book: There were a few people inside at the bar, and outside, alone, sat Harvey Stone. He had a pile of saucers in front of him, and he needed a shave. “Sit down,” said Harvey, “I’ve been looking for you.” “What’s the matter?” “Nothing. Just looking for you.” “Been out to the races?” “No. Not since Sunday.” “What do you hear from the States?” “Nothing. Absolutely nothing.” “What’s the matter?” Even by only looking for patterns one character at a time, our algorithm has reproduced plausible-looking prose with proper formatting. That is kind of amazing! We don’t have to generate text completely from scratch, either. We can seed the algorithm by supplying the first few letters and just let it find the next few letters. For fun, let’s make a fake book cover for our imaginary book by generating a new author name and a new title using the seed text of “Er”, “He”, and “The S”: Not bad! But the really mind-blowing part is that this algorithm can figure out patterns in any sequence of data. It can easily generate real-looking recipes or fake Obama speeches. But why limit ourselves human language? We can apply this same idea to any kind of sequential data that has a pattern. In 2015, Nintendo released Super Mario Maker™ for the Wii U gaming system. This game lets you draw out your own Super Mario Brothers levels on the gamepad and then upload them to the internet so you friends can play through them. You can include all the classic power-ups and enemies from the original Mario games in your levels. It’s like a virtual LEGO set for people who grew up playing Super Mario Brothers. Can we use the same model that generated fake Hemingway text to generate fake Super Mario Brothers levels? First, we need a data set for training our model. Let’s take all the outdoor levels from the original Super Mario Brothers game released in 1985: This game has 32 levels and about 70% of them have the same outdoor style. So we’ll stick to those. To get the designs for each level, I took an original copy of the game and wrote a program to pull the level designs out of the game’s memory. Super Mario Bros. is a 30-year-old game and there are lots of resources online that help you figure out how the levels were stored in the game’s memory. Extracting level data from an old video game is a fun programming exercise that you should try sometime. Here’s the first level from the game (which you probably remember if you ever played it): If we look closely, we can see the level is made of a simple grid of objects: We could just as easily represent this grid as a sequence of characters with one character representing each object: We’ve replaced each object in the level with a letter: …and so on, using a different letter for each different kind of object in the level. I ended up with text files that looked like this: Looking at the text file, you can see that Mario levels don’t really have much of a pattern if you read them line-by-line: The patterns in a level really emerge when you think of the level as a series of columns: So in order for the algorithm to find the patterns in our data, we need to feed the data in column-by-column. Figuring out the most effective representation of your input data (called feature selection) is one of the keys of using machine learning algorithms well. To train the model, I needed to rotate my text files by 90 degrees. This made sure the characters were fed into the model in an order where a pattern would more easily show up: Just like we saw when creating the model of Hemingway’s prose, a model improves as we train it. After a little training, our model is generating junk: It sort of has an idea that ‘-’s and ‘=’s should show up a lot, but that’s about it. It hasn’t figured out the pattern yet. After several thousand iterations, it’s starting to look like something: The model has almost figured out that each line should be the same length. It has even started to figure out some of the logic of Mario: The pipes in mario are always two blocks wide and at least two blocks high, so the “P”s in the data should appear in 2x2 clusters. That’s pretty cool! With a lot more training, the model gets to the point where it generates perfectly valid data: Let’s sample an entire level’s worth of data from our model and rotate it back horizontal: This data looks great! There are several awesome things to notice: Finally, let’s take this level and recreate it in Super Mario Maker: Play it yourself! If you have Super Mario Maker, you can play this level by bookmarking it online or by looking it up using level code 4AC9–0000–0157-F3C3. The recurrent neural network algorithm we used to train our model is the same kind of algorithm used by real-world companies to solve hard problems like speech detection and language translation. What makes our model a ‘toy’ instead of cutting-edge is that our model is generated from very little data. There just aren’t enough levels in the original Super Mario Brothers game to provide enough data for a really good model. If we could get access to the hundreds of thousands of user-created Super Mario Maker levels that Nintendo has, we could make an amazing model. But we can’t — because Nintendo won’t let us have them. Big companies don’t give away their data for free. As machine learning becomes more important in more industries, the difference between a good program and a bad program will be how much data you have to train your models. That’s why companies like Google and Facebook need your data so badly! For example, Google recently open sourced TensorFlow, its software toolkit for building large-scale machine learning applications. It was a pretty big deal that Google gave away such important, capable technology for free. This is the same stuff that powers Google Translate. But without Google’s massive trove of data in every language, you can’t create a competitor to Google Translate. Data is what gives Google its edge. Think about that the next time you open up your Google Maps Location History or Facebook Location History and notice that it stores every place you’ve ever been. In machine learning, there’s never a single way to solve a problem. You have limitless options when deciding how to pre-process your data and which algorithms to use. Often combining multiple approaches will give you better results than any single approach. Readers have sent me links to other interesting approaches to generating Super Mario levels: If you liked this article, please consider signing up for my Machine Learning is Fun! email list. I’ll only email you when I have something new and awesome to share. It’s the best way to find out when I write more articles like this. You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I’d love to hear from you if I can help you or your team with machine learning. Now continue on to Machine Learning is Fun Part 3! Interested in computers and machine learning. Likes to write about it. 19.3K  70  19.3K  19.3K  70  Interested in computers and machine learning. Likes to write about it. About Help Legal Get the Medium app ‘I want to learn Artificial Intelligence and Machine Learning. Where can I start?’ How did I get started? My Self-Created AI Masters Degree Getting a job Sharing your work How do you start? How much math? What does a machine learning engineer actually do? No set path Sign in Daniel Bourke Sep 29, 2018·8 min read I was working at the Apple Store and I wanted a change. To start building the tech I was servicing. I began looking into Machine Learning (ML) and Artificial Intelligence (AI). There’s so much going on. Too much. Every week it seems like Google or Facebook are releasing a new kind of AI to make things faster or improve our experience. And don’t get me started on the number of self-driving car companies. This is a good thing though. I’m not a fan of driving and roads are dangerous. Even with all this happening, there’s still yet to be an agreed definition of what exactly artificial intelligence is. Some argue deep learning can be considered AI, others will say it’s not AI unless it passes the Turing Test. This lack of definition really stunted my progress in the beginning. It was hard to learn something which had so many different definitions. Enough with the definitions. My friends and I were building a web startup. It failed. We gave up due to a lack of meaning. But along the way, I was starting to hearing more and more about ML and AI. “The computer learns things for you?” I couldn’t believe it. I stumbled across Udacity’s Deep Learning Nanodegree. A fun character called Siraj Raval was in one of the promo videos. His energy was contagious. Despite not meeting the basic requirements (I had never written a line of Python before), I signed up. 3 weeks before the course start date I emailed Udacity support asking what the refund policy was. I was scared I wouldn’t be able to complete the course. I didn’t get a refund. I completed the course within the designated timeline. It was hard. Really hard at times. My first two projects were handed in four days late. But the excitement of being involved in one of the most important technologies in the world drove me forward. Finishing the Deep Learning Nanodegree, I had guaranteed acceptance into either Udacity’s AI Nanodegree, Self-Driving Car Nanodegree or Robotics Nanodegree. All great options. I was lost again. The classic. “Where do I go next?” I needed a curriculum. I’d built a foundation with the Deep Learning Nanodegree, now it was time to figure out what was next. I didn’t plan on going back to university anytime soon. I didn’t have $100,000 for a proper Masters Degree anyway. So I did what I did in the beginning. Asked my mentor, Google, for help. I’d jumped into deep learning without any prior knowledge of the field. Instead of climbing to the tip of the AI iceberg, a helicopter had dropped me off on the top. After researching a bunch of courses, I put a list of which ones interested me the most in Trello. I knew online courses had a high drop out rate. I wasn’t going to let myself be a part of this number. I had a mission. To make myself accountable, I started sharing my learning journey online. I figured I could practice communicating what I learned plus find other people who were interested in the same things I was. My friends still think I’m an alien when I go on one of my AI escapades. I made the Trello board public and wrote a blog post about my endeavours. The curriculum has changed slightly since I first wrote it but it’s still relevant. I’d visit the Trello board multiple times per week to track my progress. I’m Australian. And all the commotion seemed to be happening in the US. So I did the most logical thing and bought a one-way ticket. I’d been studying for a year and I figured it was about time I started putting my skills into practice. My plan was to rock up to the US and get hired. Then Ashlee messaged me on LinkedIn, “Hey I’ve seen your posts and they’re really cool, I think you should meet Mike.” I met Mike. I told him my story of learning online, how I loved healthtech and my plans to go to the US. “You may be better off staying here a year or so and seeing what you can find, I’ think you’d love to meet Cameron.” I met Cameron. We had a similar chat what Mike and I talked about. Health, tech, online learning, US. “We’re working on some health problems, why don’t you come in on Thursday?” Thursday came. I was nervous. But someone once told me being nervous is the same as being excited. I flipped to being excited. I spent the day meeting the Max Kelsen team and the problems they were working on. Two Thursday’s later, Nick, the CEO, Athon, lead machine learning engineer, and I went for coffee. “How would you like to join the team?” Asked Nick. “Sure,” I said. My US flight got pushed back a couple of months and I purchased a return ticket. Learning online, I knew it was unconventional. All the roles I’d gone to apply for had Masters Degree requirements or at least some kind of technical degree. I didn’t have either of these. But I did have the skills I’d gathered from a plethora of online courses. Along the way, I was sharing my work online. My GitHub contained all the projects I’d done, my LinkedIn was stacked out and I’d practised communicating what I learned through YouTube and articles on Medium. I never handed in a resume for Max Kelsen. “We saw your LinkedIn profile.” My body of work was my resume. Regardless if you’re learning online or through a Masters Degree, having a portfolio of what you’ve worked on is a great way to build skin in the game. ML and AI skills are in demand but that doesn’t mean you don’t have to showcase them. Even the best product won’t sell without any shelf space. Whether it be GitHub, Kaggle, LinkedIn or a blog, have somewhere where people can find you. Plus, having your own corner of the internet is great fun. Where do you go to learn these skills? What courses are the best? There’s no best answer. Everyone’s path will be different. Some people learn better with books, others learn better through videos. What’s more important than how you start is why you start. Start with why. Why do you want to learn these skills? Do you want to make money? Do you want to build things? Do you want to make a difference? There’s no right reason. All are valid in their own way. Start with why because having a why is more important than how. Having a why means when it gets hard and it will get hard, you’ve got something to turn to. Something to remind you why you started. Got a why? Good. Time for some hard skills. I can only recommend what I’ve tried. I’ve completed courses from (in order): They’re all world-class. I’m a visual learner. I learn better seeing things being done. All of these courses do that. If you’re an absolute beginner, start with some introductory Python courses and when you’re a bit more confident, move into data science, machine learning and AI. DataCamp is great for beginners learning Python but wanting to learn it with a data science and machine learning focus. The highest level of math education I’ve had was in high school. The rest I’ve learned through Khan Academy as I’ve needed it. There are many different opinions on how much math you need to know to get into machine learning and AI. I’ll share mine. If you want to apply machine learning and AI techniques to a problem, you don’t necessarily need an in-depth understanding of the math to get a good result. Libraries such as TensorFlow and PyTorch allow someone with a bit of Python experience to build state of the art models whilst the math is taken care of behind the scenes. If you’re looking to get deep into machine learning and AI research, through means of a PhD program or something similar, having an in-depth knowledge of the math is paramount. In my case, I’m not looking to dive deep into the math and improve an algorithm’s performance by 10%. I’ll leave that to people smarter than me. Instead, I’m more than happy to use the libraries available and manipulate them to help solve problems as I see fit. What a machine engineer does in practice might not be what you think. Despite the cover photos of many online articles, it doesn’t always involve working with robots that have red eyes. Here are a few questions a machine learning engineer has to ask themselves daily. I borrowed these from a great article by Rachel Thomas, one of the co-founders of fast.ai, she goes into more depth in the full text. For more, I made a video of what we usually get up to on Monday’s at Max Kelsen. There’s no right or wrong way to get into ML or AI (or anything else). The beautiful thing about this field is we have access to some of the best technologies in the world, all we’ve got to do is learn how to use them. You could begin by learning Python code (my favourite). You could begin by studying calculus and statistics. You could begin by learning about the philosophy of decision making. Machine learning and AI fascinate me because they meet at the intersection of all of these. The more I learn about it, the more I realise there’s plenty more to learn. And it gets me excited. Sometimes I get frustrated when my code doesn’t run. Or I don’t understand a concept. So I give up temporarily. I give up by letting myself walk away from the problem and take a nap. Or go for a walk. When I come back it feels like I’m looking at it with different eyes. The excitement comes back. I keep learning. I tell myself. I’m a learning machine. There’s so much happening in the field it can be daunting to get started. Too many options lead to no options. Ignore this. Start wherever interests you most and follow it. If it leads to a dead end, great, you’ve figured out what you’re not interested in. Retrace your steps and take the other fork in the road instead. Computers are smart but they still can’t learn on their own. They need your help. I play at the crossroads of technology, health and art. Broadcasting from: www.mrdbourke.com 17.3K  73  Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss. Take a look.  By signing up, you will create a Medium account if you don’t already have one. Review our Privacy Policy for more information about our privacy practices. Check your inboxMedium sent you an email at  to complete your subscription. 17.3K  17.3K  73  Your home for data science. A Medium publication sharing concepts, ideas and codes. About Help Legal Get the Medium app \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lVnGf6tusw55"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Q--Wr1xse-z"},"source":["### **4. Read content from Arxiv with any given search query**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JrYk3N4R0zzn","executionInfo":{"status":"ok","timestamp":1616177833411,"user_tz":420,"elapsed":41700,"user":{"displayName":"Viet Hoang Tran Duong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT3sg-hdUPlX9VamfzW6qUngDvYX0YJmdFBMNS5Q=s64","userId":"14828536041389094926"}},"outputId":"ae23ac1a-0e81-4f6c-a15f-5703f92bc8e6"},"source":["output, text = get_arxiv(\"transformers\", search_type=\"Title\", size=25)\n","\n","print(\"Number of posts crawled:\", len(output))\n","print(\"Example of one crawled information:\", output[0])\n","print(\"\")\n","print(\"All the text of all posts (combined):\", text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of posts crawled: 25\n","Example of one crawled information: ['https://arxiv.org/abs/2103.02181', 'Reflectionless Plasmonic Right-Angled Waveguide Bend and Divider Using Graphene and Transformation Optics', 'Using the Transformation Optics (TO) approach the transformation media of a bend and a T-shaped divider are obtained. Such media with continuous refractive index are realized with the help of graphene in the terahertz frequency range, key to effectively guiding the surface plasmon polariton (SPP) propagation on the 90 degree curves.']\n","\n","All the text of all posts (combined): Reflectionless Plasmonic Right-Angled Waveguide Bend and Divider Using Graphene and Transformation Optics Using the Transformation Optics (TO) approach the transformation media of a bend and a T-shaped divider are obtained. Such media with continuous refractive index are realized with the help of graphene in the terahertz frequency range, key to effectively guiding the surface plasmon polariton (SPP) propagation on the 90 degree curves. TransCT: Dual-path Transformer for Low Dose Computed Tomography After that, we combine these well-refined HF texture features with the pre-extracted X_{L_c} to encourage the restoration of high-quality LDCT images with the assistance of piecewise reconstruction. However, the noise caused by low X-ray exposure degrades the CT image quality and then affects clinical diagnosis accuracy. To be specific, we first decompose the noisy LDCT image into two parts: high-frequency (HF) and low-frequency (LF) compositions. Further, we feed X_{L_t} and X_{H_f} into a modified transformer with three encoders and decoders to obtain well-refined HF texture features. Single-Shot Motion Completion with Transformer In this work, we propose a simple but effective method to solve multiple motion completion problems under a unified framework and achieves a new state of the art accuracy under multiple evaluation settings. For different motion completion scenarios (in-betweening, in-filling, and blending), most previous methods deal with the completion problems with case-by-case designs. Motion completion is a challenging and long-discussed problem, which is of great significance in film and game applications. An Introduction to Johnson-Lindenstrauss Transforms This note explains what they are; it gives an overview of their use and their development since they were introduced in the 1980s; and it provides many references should the reader wish to explore these topics more deeply. Generative chemical transformer: attention makes neural machine learn molecular geometric structures via text Here, we propose a neural machine that creates molecules that meet some desired conditions based on a deep understanding of chemical language (generative chemical Transformer, GCT). Attention-mechanism in GCT allows a deeper understanding of molecular structures, beyond the limitations of chemical language itself that cause semantic discontinuity, by paying attention to characters sparsely. We investigate the significance of language models to inverse molecular design problems by quantitatively evaluating the quality of generated molecules. GCT generates highly realistic chemical strings that satisfy both a chemical rule and grammars of a language. All-optical input-agnostic polarization transformer We demonstrate an all-optical input-agnostic polarization transformer (AI-APT), which transforms all input states of polarization to a particular state that can be polarized or partially polarized. The output state of polarization and intensity depends solely on setup parameters, and not on the input state, thereby the AI-APT functions differently from simple polarizers and polarization rotators. Simple polarizers change the intensity depending on the input state and can only output a fixed polarized state, while polarization rotators rotates the input Stokes vector in the 3D Stokes space. The AI-APT is completely passive, and thus can be used as a polarization controller or stabilizer for single photons and ultrafast pulses. ReadNet: A Hierarchical Transformer Framework for Web Article Readability Analysis Current methods for assessing readability employ empirical measures or statistical learning techniques that are limited by their ability to characterize complex patterns such as article structures and semantic meanings of sentences. Experimental results show that our proposed method achieves the state-of-the-art performance on estimating the readability for various web articles and literature. Additionally, the sentence-level features are incorporated to characterize the overall readability of an article with consideration of article structures. Addressing this task is necessary to the automatic recommendation of appropriate articles to readers with different comprehension abilities, and it further benefits education systems, web information systems, and digital libraries. Perspectives and Prospects on Transformer Architecture for Cross-Modal Tasks with Language and Vision Its success also implies drastic changes in cross-modal tasks with language and vision, and many researchers have already tackled the issue. In this paper, we review some of the most critical milestones in the field, as well as overall trends on how transformer architecture has been incorporated into visuolinguistic cross-modal tasks. CrossMap Transformer: A Crossmodal Masked Path Transformer Using Double Back-Translation for Vision-and-Language Navigation This task involves the prediction of a sequence of actions that leads to a specified destination given a natural language navigation instruction. The two networks share common latent features, for mutual enhancement through a double back translation model: Generated paths are translated into instructions while generated instructions are translated into path The experimental results show the benefits of our approach in terms of instruction understanding and instruction generation. The Visual and Language Navigation remains challenging, notably because it requires the exploration of the environment and at the accurate following of a path specified by the instructions to model the relationship between language and vision. Enhancing Transformation-based Defenses against Adversarial Examples with First-Order Perturbations We observe that the probability of the correct result outputted by the neural network increases by applying small perturbations generated for non-predicted class labels to adversarial examples. The experimental results demonstrate that our method effectively improves the defense performance of the baseline methods, especially against strong adversarial examples generated using more iterations. The obtained perturbation is finally added to the adversarial example to counteract the adversarial perturbation contained in the example. The proposed method is applied at inference time and does not require retraining or finetuning the model. Based on this observation, we propose a method of counteracting adversarial perturbations to resist adversarial examples. End-to-End Human Object Interaction Detection with HOI Transformer We propose HOI Transformer to tackle human object interaction (HOI) detection in an end-to-end manner. In contrast, our method, named HOI Transformer, streamlines the HOI pipeline by eliminating the need for many hand-designed components. HOI Transformer reasons about the relations of objects and humans from global image context and directly predicts HOI instances in parallel. Current approaches either decouple HOI task into separated stages of object detection and interaction classification or introduce surrogate interaction problem. Generative Adversarial Transformers The network employs a bipartite structure that enables long-range interactions across the image, while maintaining computation of linearly efficiency, that can readily scale to high-resolution synthesis. It iteratively propagates information from a set of latent variables to the evolving visual features and vice versa, to support the refinement of each in light of the other and encourage the emergence of compositional representations of objects and scenes. In contrast to the classic transformer architecture, it utilizes multiplicative integration that allows flexible region-based modulation, and can thus be seen as a generalization of the successful StyleGAN network. OmniNet: Omnidirectional Representations from Transformers To this end, the omnidirectional attention is learned via a meta-learner, which is essentially another self-attention based model. This process can also be interpreted as a form of extreme or intensive attention mechanism that has the receptive field of the entire width and depth of the network. Moreover, using omnidirectional representation in Vision Transformers leads to significant improvements on image recognition tasks on both few-shot learning and fine-tuning setups. Extensive experiments are conducted on autoregressive language modeling (LM1B, C4), Machine Translation, Long Range Arena (LRA), and Image Recognition. In order to mitigate the computationally expensive costs of full receptive field attention, we leverage efficient self-attention models such as kernel-based (Choromanski et al. Zero-Shot Cross-Lingual Dependency Parsing through Contextual Embedding Transformation Experiments on zero-shot dependency parsing through the concept-shared space built by our embedding transformation substantially outperform state-of-the-art methods using multilingual embeddings. We move further from this line and investigate a contextual embedding alignment approach which is sense-level and dictionary-free. Changes in electric-field noise due to thermal transformation of a surface ion trap The underlying physics of this thermal transformation process is explored through a series of heating, milling, and electron treatments performed on a single surface ion trap. We hypothesize that a rise in local atomic order increases surface electric-field noise in this system. The data suggest that the observed thermal transformations can be explained by atomic restructuring at the trap surface. In our system, prolonged heat treatments of a metal film can induce a rise in the magnitude of the electric-field noise generated by the surface of that film. An atomic perspective on the serpentine-chlorite solid-state transformation Serpentine minerals are important components of metamorphic rocks and promising geo-materials for nanotechnology. Further, we show that different serpentine polytypes result in distinct regular interstratifications of serpentine and chlorite. Here we visualized cations and oxygen atoms using the state-of-the-art low-dose scanning transmission electron microscopy and found that restructuring mainly involves the synergistic migration of tetrahedral cations and oxygen anions, coupled with the migration of octahedral trivalent cations into the brucite-like interlayer. Generative Transition Mechanism to Image-to-Image Translation via Encoded Transformation In this paper, we revisit the Image-to-Image (I2I) translation problem with transition consistency, namely the consistency defined on the conditional data mapping between each data pairs. With the above design, our proposed model, named Transition Encoding GAN (TEGAN), can poss superb generalization ability to generate realistic and semantically consistent translation results with unseen transitions in the test phase. To benefit the generalization ability of the translation model, we propose transition encoding to facilitate explicit regularization of these two {kinds} of consistencies on unseen transitions. Explicitly parameterizing each data mappings with a transition variable $t$, i.e., $x \\overset{t(x,y)}{\\mapsto}y$, we discover that existing I2I translation models mainly focus on maintaining consistency on results, e.g., image reconstruction or attribute prediction, named result consistency in our paper. Cascaded Filtering Using the Sigma Point Transformation (Extended Version) The proposed approach is compared to a naive cascaded filtering approach that neglects cross-covariance terms, a sigma point-based Covariance Intersection filter, and a full-state filter. Common cascaded filtering techniques focus on the problem of modelling cross-covariances when the local estimators share a common state vector. This letter introduces a novel cascaded and decentralized filtering approach that approximates the cross-covariances when the local estimators consider distinct state vectors. On Security Properties of All-or-nothing Transforms These mappings are required to satisfy certain \"perfect security\" conditions specified using entropies of the probability distribution defined on the input s-tuples. However, the combinatorial definition makes no reference to probability definitions. The security of the AONT can depend on the underlying probability distribution of the s-tuples. Alternatively, purely combinatorial definitions of AONTs have been given, which involve certain kinds of \"unbiased arrays\". U-Net Transformer: Self and Cross Attention for Medical Image Segmentation To this end, attention mechanisms are incorporated at two main levels: a self-attention module leverages global interactions between encoder features, while cross-attention in the skip connections allows a fine spatial recovery in the U-Net decoder by filtering out non-semantic features. We also highlight the importance of using both self- and cross-attention, and the nice interpretability features brought out by U-Transformer. In this paper, we introduce the U-Transformer network, which combines a U-shaped architecture for image segmentation with self- and cross-attention from Transformers. Bell polynomials and generalized Laplace transforms Furthermore, the solution of the Blissard problem by means of the Bell polynomials, gives the possibility to associate to any numerical sequence a Laplace-type transform depending on that sequence. A Multi-Modal Transformer-based Code Summarization Approach for Smart Contracts The MMTrans uses two encoders to extract both global and local semantic information from the two modalities respectively, and then uses a joint decoder to generate code comments. Code comment has been an important part of computer programs, greatly facilitating the understanding and maintenance of source code. We build a dataset with over 300K <method, comment> pairs of smart contracts, and evaluate the MMTrans on it. Both the encoders and the decoder employ the multi-head attention structure of the Transformer to enhance the ability to capture the long-range dependencies between code tokens. TransMed: Transformers Advance Multi-modal Medical Image Classification However, medical imaging datasets are relatively small, which makes it difficult to apply pure transformers to medical image analysis. Compared with natural images, multi-modal medical images have explicit and important long-range dependencies, and effective multi-modal fusion strategies can greatly improve the performance of deep models. TransMed combines the advantages of CNN and transformer to efficiently extract low-level features of images and establish long-range dependencies between modalities. We argue that the combination of CNN and transformer has tremendous potential in a large number of medical image analysis tasks. Existing transformer-based network architectures require large-scale datasets to achieve better performance. Recently, transformers have been applied to computer vision and achieved remarkable success in large-scale datasets. Context Transformer with Stacked Pointer Networks for Conversational Question Answering over Knowledge Graphs In this work, we propose a novel framework named CARTON, which performs multi-task semantic parsing for handling the problem of conversational question answering over a large-scale knowledge graph. Our framework consists of a stack of pointer networks as an extension of a context transformer model for parsing the input question and the dialog history. The framework generates a sequence of actions that can be executed on the knowledge graph. Neural semantic parsing approaches have been widely used for Question Answering (QA) systems over knowledge graphs. Minimality and unique ergodicity of Veech 1969 type interval exchange transformations We provide also a word combinatorial criterion of minimality valid for general interval exchange transformations, which applies to $\\mathbb Z/N\\mathbb Z$ extensions of any interval exchange transformation with any number of marked points. \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0rrVwj9bsxT7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yMYqa_zEsjjx"},"source":["### **5. Read content from NeurIPS with any given search query**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7Chxd6q1IM4","executionInfo":{"status":"ok","timestamp":1616177837555,"user_tz":420,"elapsed":45827,"user":{"displayName":"Viet Hoang Tran Duong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiT3sg-hdUPlX9VamfzW6qUngDvYX0YJmdFBMNS5Q=s64","userId":"14828536041389094926"}},"outputId":"e4c0747c-660a-4d65-ff83-52b0b314b5ae"},"source":["output, text = get_neurips(\"convolutional\")\n","\n","print(\"Number of posts crawled:\", len(output))\n","print(\"Example of one crawled information:\", output[0])\n","print(\"\")\n","print(\"All the text of all posts (combined):\", text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of posts crawled: 10\n","Example of one crawled information: ['2020', 'https://proceedings.neurips.cc//paper/2020/hash/0e1ebad68af7f0ae4830b7ac92bc3c6f-Abstract.html', 'ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks', 'We introduce an approach to training a given compact network. To this end, we leverage over-parameterization, which typically improves both neural network optimization and generalization. Specifically, we propose to expand each linear layer of the compact network into multiple consecutive linear layers, without adding any nonlinearity. As such, the resulting expanded network, or ExpandNet, can be contracted back to the compact one algebraically at inference. In particular, we introduce two convolutional expansion strategies and demonstrate their benefits on several tasks, including image classification, object detection, and semantic segmentation. As evidenced by our experiments, our approach outperforms both training the compact network from scratch and performing knowledge distillation from a teacher. Furthermore, our linear over-parameterization empirically reduces gradient confusion during training and improves the network generalization.']\n","\n","All the text of all posts (combined): ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks We introduce an approach to training a given compact network. To this end, we leverage over-parameterization, which typically improves both neural network optimization and generalization. Specifically, we propose to expand each linear layer of the compact network into multiple consecutive linear layers, without adding any nonlinearity. As such, the resulting expanded network, or ExpandNet, can be contracted back to the compact one algebraically at inference. In particular, we introduce two convolutional expansion strategies and demonstrate their benefits on several tasks, including image classification, object detection, and semantic segmentation. As evidenced by our experiments, our approach outperforms both training the compact network from scratch and performing knowledge distillation from a teacher. Furthermore, our linear over-parameterization empirically reduces gradient confusion during training and improves the network generalization. Digraph Inception Convolutional Networks Graph Convolutional Networks (GCNs) have shown promising results in modeling graph-structured data. However, they have difficulty with processing digraphs because of two reasons: 1) transforming directed to undirected graph to guarantee the symmetry of graph Laplacian is not reasonable since it not only misleads message passing scheme to aggregate incorrect weights but also deprives the unique characteristics of digraph structure; 2) due to the fixed receptive field in each layer, GCNs fail to obtain multi-scale features that can boost their performance. In this paper, we theoretically extend spectral-based graph convolution to digraphs and derive a simplified form using personalized PageRank. Specifically, we present the Digraph Inception Convolutional Networks (DiGCN) which utilizes digraph convolution and kth-order proximity to achieve larger receptive fields and learn multi-scale features in digraphs. We empirically show that DiGCN can encode more structural information from digraphs than GCNs and help achieve better performance when generalized to other models. Moreover, experiments on various benchmarks demonstrate its superiority against the state-of-the-art methods.  Convergence and Stability of Graph Convolutional Networks on Large Random Graphs We study properties of Graph Convolutional Networks (GCNs) by analyzing their behavior on standard models of random graphs, where nodes are represented by random latent variables and edges are drawn according to a similarity kernel.  This allows us to overcome the difficulties of dealing with discrete notions such as isomorphisms on very large graphs, by considering instead more natural geometric aspects. We first study the convergence of GCNs to their continuous counterpart as the number of nodes grows. Our results are fully non-asymptotic and are valid for relatively sparse graphs with an average degree that grows logarithmically with the number of nodes. We then analyze the stability of GCNs to small deformations of the random graph model. In contrast to previous studies of stability in discrete settings, our continuous setup allows us to provide more intuitive deformation-based metrics for understanding stability, which have proven useful for explaining the success of convolutional representations on Euclidean domains. The Origins and Prevalence of Texture Bias in Convolutional Neural Networks Recent work has indicated that, unlike humans, ImageNet-trained CNNs tend to classify images by texture rather than by shape. How pervasive is this bias, and where does it come from? We find that, when trained on datasets of images with conflicting shape and texture, CNNs learn to classify by shape at least as easily as by texture. What factors, then, produce the texture bias in CNNs trained on ImageNet? Different unsupervised training objectives and different architectures have small but significant and largely independent effects on the level of texture bias. However, all objectives and architectures still lead to models that make texture-based classification decisions a majority of the time, even if shape information is decodable from their hidden representations. The effect of data augmentation is much larger. By taking less aggressive random crops at training time and applying simple, naturalistic augmentation (color distortion, noise, and blur), we train models that classify ambiguous images by shape a majority of the time, and outperform baselines on out-of-distribution test sets. Our results indicate that apparent differences in the way humans and ImageNet-trained CNNs process images may arise not primarily from differences in their internal workings, but from differences in the data that they see. Factorizable Graph Convolutional Networks Graphs have been widely adopted to denote structural connections between entities. The relations are in many cases heterogeneous, but entangled together and denoted merely as a single edge between a pair of nodes. For example, in a social network graph, users in different latent relationships like friends and colleagues, are usually connected via a bare edge that conceals such intrinsic connections. In this paper, we introduce a novel graph convolutional network (GCN), termed as factorizable graph convolutional network (FactorGCN), that explicitly disentangles such intertwined relations encoded in a graph. FactorGCN takes a simple graph as input, and disentangles it into several factorized graphs, each of which represents a latent and disentangled relation among nodes. The features of the nodes are then aggregated separately in each factorized latent space to produce disentangled features, which further leads to better performances for downstream tasks. We evaluate the proposed FactorGCN both qualitatively and quantitatively on the synthetic and real-world datasets, and demonstrate that it yields truly encouraging results in terms of both disentangling and feature aggregation. Code is publicly available at https://github.com/ihollywhy/FactorGCN.PyTorch.  Reward Propagation Using Graph Convolutional Networks Potential-based reward shaping provides an approach for designing good reward functions, with the purpose of speeding up learning. However, automatically finding potential functions for complex environments is a difficult problem (in fact, of the same difficulty as learning a value function from scratch). We propose a new framework for learning potential functions by leveraging ideas from graph representation learning. Our approach relies on Graph Convolutional Networks which we use as a key ingredient in combination with the probabilistic inference view of reinforcement learning. More precisely, we leverage Graph Convolutional Networks to perform message passing from rewarding states. The propagated messages can then be used as potential functions for reward shaping to accelerate learning. We verify empirically that our approach can achieve considerable improvements in both small and high-dimensional control problems. Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting Modeling complex spatial and temporal correlations in the correlated time series data is indispensable for understanding the traffic dynamics and predicting the future status of an evolving traffic system. Recent works focus on designing complicated graph neural network architectures to capture shared patterns with the help of pre-defined graphs. In this paper, we argue that learning node-specific patterns is essential for traffic forecasting while pre-defined graph is avoidable.\n","To this end, we propose two adaptive modules for enhancing Graph Convolutional Network (GCN) with new capabilities: 1) a Node Adaptive Parameter Learning (NAPL) module to capture node-specific patterns; 2) a Data Adaptive Graph Generation (DAGG) module to infer the inter-dependencies among different traffic series automatically. We further propose an Adaptive Graph Convolutional Recurrent Network (AGCRN) to capture fine-grained spatial and temporal correlations in traffic series automatically based on the two modules and recurrent networks. Our experiments on two real-world traffic datasets show AGCRN outperforms state-of-the-art by a significant margin without pre-defined graphs about spatial connections. How to Characterize The Landscape of Overparameterized Convolutional Neural Networks For many initialization schemes, parameters of two randomly initialized deep neural networks (DNNs) can be quite different, but feature distributions of the hidden nodes are similar at each layer. With the help of a new technique called {\\it neural network grafting}, we demonstrate that even during the entire training process, feature distributions of differently initialized networks remain similar at each layer. In this paper, we present an explanation of this phenomenon. Specifically, we consider the loss landscape of an overparameterized convolutional neural network (CNN) in the continuous limit, where the numbers of channels/hidden nodes in the hidden layers go to infinity. Although the landscape of the overparameterized  CNN is still non-convex with respect to the trainable parameters, we show that very surprisingly, it can be reformulated as a convex function with respect to the feature distributions in the hidden layers. Therefore by reparameterizing neural networks in terms of feature distributions, we obtain a much simpler characterization of the landscape of overparameterized CNNs. We further argue that training with respect to network parameters leads to a fixed trajectory in the feature distributions. Scattering GCN: Overcoming Oversmoothness in Graph Convolutional Networks Graph convolutional networks (GCNs) have shown promising results in processing graph data by extracting structure-aware features. This gave rise to extensive work in geometric deep learning, focusing on designing network architectures that ensure neuron activations conform to regularity patterns within the input graph. However, in most cases the graph structure is only accounted for by considering the similarity of activations between adjacent nodes, which limits the capabilities of such methods to discriminate between nodes in a graph. Here, we propose  to augment conventional GCNs with geometric scattering transforms and residual convolutions. The former enables band-pass filtering of graph signals, thus alleviating the so-called oversmoothing often encountered in GCNs, while the latter is introduced to clear the resulting features of high-frequency noise. We establish the advantages of the presented Scattering GCN with both theoretical results establishing the complementary benefits of scattering and GCN features, as well as experimental results showing the benefits of our method compared to leading graph neural networks for semi-supervised node classification, including the recently proposed GAT network that typically alleviates oversmoothing using graph attention mechanisms. Fully Convolutional Mesh Autoencoder using Efficient Spatially Varying Kernels Learning latent representations of registered meshes is useful for many 3D tasks. Techniques have recently shifted to neural mesh autoencoders. Although they demonstrate higher precision than traditional methods, they remain unable to capture fine-grained deformations. Furthermore, these methods can only be applied to a template-specific surface mesh, and is not applicable to more general meshes, like tetrahedrons and non-manifold meshes. While more general graph convolution methods can be employed, they lack performance in reconstruction precision and require higher memory usage. \n","In this paper, we propose a non-template-specific fully convolutional mesh autoencoder for arbitrary registered mesh data. It is enabled by our novel convolution and (un)pooling operators learned with globally shared weights and locally varying coefficients which can efficiently capture the spatially varying contents presented by irregular mesh connections. \n","Our model outperforms state-of-the-art methods on reconstruction accuracy. In addition, the latent codes of our network are fully localized thanks to the fully convolutional structure, and thus have much higher interpolation capability than many traditional 3D mesh generation models. \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tACJ5t1asx1w"},"source":[""],"execution_count":null,"outputs":[]}]}